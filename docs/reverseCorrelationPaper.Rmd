---
title             : "Paradoxical evidence weighting in confidence judgments for detection and discrimination"
shorttitle        : "Paradoxical evidence weighting in confidence judgments for detection and discrimination"

author: 
  - name          : "Matan Mazor"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "12 Queen Square, London WC1N 3BG"
    email         : "mtnmzor@gmail.com"
  - name          : "Lucie Charles"
    affiliation   : "4"
  - name          : "Roni Maimon-Mor"
    affiliation   : "4" 
  - name          : "Stephen M. Fleming"
    affiliation   : "1,2,3"

affiliation:
  - id            : "1"
    institution   : "Wellcome Centre for Human Neuroimaging, UCL"
  - id            : "2"
    institution   : "Max Planck UCL Centre for Computational Psychiatry and Ageing Research"
  - id            : "3"
    institution   : "Department of Experimental Psychology, UCL"
  - id            : "4"
    institution   : "Institute of Cognitive Neuroscience, UCL"

authornote: |
  The authors have no conflicting interests to declare. This research was funded in whole, or in part, by the Wellcome Trust [Grant number]. For the purpose of Open Access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.’

abstract: | 
  When making discrimination decisions between two stimulus categories, subjective confidence judgments are more positively affected by evidence in support of a decision than negatively affected by evidence against it. Recent theoretical proposals suggest that this “positive evidence bias” may be due to observers adopting a detection-like strategy when rating their confidence, one that has functional benefits for metacognition in real-world settings where detectability and discriminability often go hand in hand. However, it is unknown whether, or how, this evidence weighting asymmetry affects detection decisions about the presence or absence of a stimulus. In four experiments we first successfully replicate a positive evidence bias in discrimination confidence. We then show that detection decisions and confidence ratings paradoxically suffer from an opposite “negative evidence bias” to negatively weigh evidence even when it is optimal to assign it a positive weight. We show that the two effects are uncorrelated, and discuss our findings in relation to models that account for a positive evidence bias as emerging from a confidence-specific heuristic, and alternative models where decision and confidence are generated by the same, Bayes-rational process.

  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "confidence, detection, metacognition"
wordcount         : "X"

bibliography      : ["RC.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_word
appendix          : appendix.rmd   


---

```{r RC_load_pkgs, echo=FALSE, message=FALSE, include=FALSE}
library(groundhog)

groundhog.library(
  c(
    'papaja',
    'tidyverse',
    'broom',
    'cowplot',
    'MESS', # for AUCs
    'lsr', # for effect sizes
    'pwr', # for power calculations
    'brms', # for mixed effects modeling
    'BayesFactor', # for Bayesian t test
    'jsonlite', #parsing data from sort_trial
    'knitr',
    'egg',
    'zoo',  # for rollapply
    'RColorBrewer',
    'reticulate', # for python
    'pracma' #for absolute least squares
  ),"2022-12-01"
)

r_refs("r-references.bib")

source("trialLevelAnalysisFunctions.R", local = knitr::knit_global())
source("visualizationFunctions.R", local = knitr::knit_global())
source("reverseCorrelationFunctions.R", local = knitr::knit_global())

```

# Introduction

When considering two alternative hypotheses, the probability of the chosen hypothesis being correct is a function of the availability of evidence supporting not only the chosen hypothesis, but also the unchosen one. For example, when deciding that there are more ants in the kitchen than in the living room, confidence should not only positively weigh the number of ants found in the kitchen (*positive evidence*), but also negatively weigh the number of ants found in the living room (*negative evidence*). Specifically, a decision should be based on the difference in the number of ants between the kitchen and the living room, but not on the total number of ants found in both rooms together (we refer to these quantities as *relative evidence* and *sum evidence*, respectively).

While sum evidence is irrelevant to discrimination decisions between two symmetrical hypotheses (e.g., kitchen or living room), it is highly informative with respect to detection decisions about the presence or absence of a signal. For example, when deciding that an ant colony is nesting in the house, we should also care about the total number of ants, irrespective whether they are found in the kitchen or living room (see Fig. \@ref(fig:RC-2d-models)).

(ref:RC-2dmodels) Discrimination and detection in a two-dimensional Signal Detection Theory model. Left: in a two-dimensional SDT model, evidence $e$ is sampled from one of two Gaussian distributions (here centered at (0,1) and (1,0)). We define relative evidence as $e_{S1}-e_{S2}$ and sum evidence as $e_{S1}+e_{S2}$. Circles represent contours of two-dimensional distributions. Center and Left: decision and confidence accuracy are maximized when based on a log-likelihood ratio for the two stimulus categories. Center: in discrimination, this yields optimal decision and confidence criteria that are based on relative evidence (distance from the main diagonal), irrespective of sum evidence. Right: in detection, this yields optimal decision and confidence that are based on a non-linear interaction between relative and sum evidence. The third circle centred at (0,0) represents the two-dimensional distribution of percepts in the absence of stimuli.

```{r RC-2d-models, fig.cap="(ref:RC-2dmodels)"}

knitr::include_graphics('figures/2dmodel_enhanced.png')

```



A surprising finding is that, despite the irrelevance of sum evidence to the accuracy of discrimination decisions, people are systematically more confident in their perceptual discrimination decisions when sum evidence is high. For example, @zylberberg2012construction had subjects judge which of two flickering stimuli was brighter on average. Subjects were more confident in their decisions when both stimuli were brighter, indicating an effect of sum evidence (here, overall luminance) on decision confidence. A positive effect of sum evidence on decision confidence is mathematically equivalent to a disproportional weighting of positive evidence over negative evidence, also known as a positive evidence bias [@koizumi2015does; @peters2017perceptual; @rollwage2020confidence; @samaha2020positive; @sepulveda2020visual; @zylberberg2012construction]. The two are equivalent because positively weighing the sum of positive and negative evidence effectively weakens the negative contribution of negative evidence to decision confidence, while strengthening the contribution of positive evidence. Notably, this finding stands in contrast to what is expected from the exponential scaling of sensory noise relative to stimulus energy (Weber’s law). Instead, an effect of sum evidence on discrimination confidence may indicate a profound link between how confidence is formed in general, and the processes underpinning perceptual detection [@rausch2018confidence; @samaha2020spontaneous].

Different models identify the origin of this evidence weighting asymmetry at different levels of the cognitive hierarchy, ranging from positing a metacognitive bias that ignores conflicting information [metacognitive level, @peters2017perceptual; @maniscalco2016heuristic], to asymmetries in the active sampling of evidence [attention allocation level, @sepulveda2020visual], and down to perceptual asymmetries between the representations of signal and noise [perception level, @miyoshi2020decision; @webb2021task]. These models vary in whether they postulate separate evidence accumulation processes for decisions and confidence judgments, and in whether they model confidence formation as following a suboptimal heuristic, or alternatively as being optimal with respect to available information (information which may be limited or corrupted by noise).

Here we focus on a subset of models which assume that subjects are rational decision makers equipped with veridical beliefs about the world, but who only have limited access to noisy evidence. Our models further assume that subjects’ confidence ratings are Bayesian estimates of the probability of being correct, given the exact same evidence that was used to make the decision. The models do not postulate any metacognitive biases, heuristics, or suboptimalities. We show that two of these models reproduce a positive evidence bias (that is, a positive effect of sum evidence) in discrimination confidence. The same models also make predictions for evidence weighting in detection judgments and confidence ratings. In four experiments, reverse correlation analysis revealed evidence weighting patterns that only partly agree with the predictions of our models. Most notably, our four models fail to account for a negative evidence bias we observed in detection decisions and confidence: a tendency to irrationally place a negative weighting on evidence, such as being more confident in the presence of a bright stimulus when one of the presented stimuli was unusually dark. In what follows we first describe the four models and the predictions they make, before turning to empirical findings from our four experiments.

# Computational models

We model a setting in which agents are presented with a sequence of samples from two noisy sensory channels: $E_1$ and $E_2$. The agents’ task is to decide which of the two channels was the signal channel (discrimination), or whether any of the channels had signal in it at all (detection). When a signal is present in a channel, evidence E is sampled from a normal distribution $\mathcal{N}(0.5,1)$, and when a signal is absent evidence is sampled from $\mathcal{N}(0,1)$ (see Fig. \@ref(fig:RC-models), upper panel). In all four models agents only have access to a noisy version of these samples $E'$, corrupted by additional internal sensory noise. After each time step, they update their belief about the relative likelihood of the observed samples under the two possible world states (signal in channel 1 versus 2, or signal presence versus absence), and given full knowledge of the true sample-generating process, including the properties of sensory noise. Each trial comprises 12 time steps. At the end of a trial, agents report the world state that maximizes the likelihood of the observed evidence, and rate their confidence as the objective probability that their decision was correct given the accumulated likelihood estimates. The four models vary in the properties of sensory noise, and in the selection of some channels for inspection by selection mechanisms.

(ref:RC-models) Computational models. Upper panel: True world model. Stimuli span 12 timepoints, each comprising values from two sensory channels (here presented as luminance values). In discrimination blocks, values in one channel are sampled from the noise distribution (red), and values in the other channel are sampled from the signal distribution (blue). In detection blocks, on half of the trials all values are sampled from the noise distribution (red). Vanilla model: on each timepoint, participants perceive both channels, corrupted by sensory noise that is sampled from a normal distribution. They then update their beliefs accordingly. Firing rate model: sensory samples are sampled from a Poisson distribution. Random attention model: agents only attend one channel at a time. The attended channel is chosen at random per timepoint, with a strong bias which is consistent within a trial. Goal-directed attention model: channels that are likely to include signal (as determined by previous samples) are more likely to be attended.

```{r RC-models, fig.cap="(ref:RC-models)"}

knitr::include_graphics('figures/models2.png')

```

## Vanilla model

In the basic, vanilla model, sensory noise is sampled from a normal distribution $\mathcal{N}(0,2)$. This model corresponds to a standard equal-variance signal detection model, as illustrated in Fig. \@ref(fig:RC-2d-models).

## Firing rate model

The firing rate model is similar to the vanilla model, with the exception that perceived values are sampled from a Poisson, rather than a normal distribution. An important property of the Poisson distribution family, commonly used to model firing rates in neuronal populations, is that their mean and variance are lawfully coupled: the stronger the activation, the more variable it is. When applied to sensory neurons, this results in strong stimuli being subjectively perceived as noisier, consistent with the Weber-Fechner law [@fechner1860elements]. In identifying the origin of the positive evidence bias at the perceptual level, this model shares a family resemblance with the unequal-variance model by @miyoshi2020decision. An important feature of this model is that perceptual noise is conditioned not on stimulus class, but on the perceptual sample. This seems plausible, as the perceptual system has no access to stimulus class beyond the information that is available in perceptual samples. 

## Random attention model

Like the vanilla model, sensory noise is again sampled from $\mathcal{N}(0,2)$. Unlike the vanilla model, however, here agents have access to one channel per timepoint only (they “attend” to one channel at a time). At the start of each trial, agents randomly choose a preferred channel. Then, on each timepoint, they attend to the preferred channel with probability 0.95, and the non-preferred channel with probability 0.05, and update their beliefs accordingly. We include this model because it is inherently asymmetric: on each trial, evidence from the preferred channel contributes more to both decision and confidence, simply because it is more visible to the agent.

## Goal-directed attention model

This model is similar to the random attention model, except that here attention is biased towards channels that are more likely to include signal. Specifically, agents track the log likelihood ratio $LLR_r$ between signal presence in the left or in the right channels, with the probability of attending the right channel being dynamically set at each timepoint to $S(LLR_r)$ where $S$ is a sigmoid function with a steep slope of 5 and $LLR_r$ is based on all previous sensory samples in the trial. A conceptually similar drift diffusion model was previously shown to produce a positive evidence bias in confidence ratings [@sepulveda2020visual].

```{r RC-analyze-simulations, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE,fig.show='hide'}

sim = list()

sim$df <- read_csv('../simulations/equal_variance/discrimination.csv') %>%
  mutate(detection=0, eccentricity='ev', signal=1, subj_id=floor(trial_id/100)) %>%
  dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                eccentricity,evidence,decision,confidence, correct) %>%
  rbind(read_csv('../simulations/equal_variance/detection.csv') %>%
  mutate(detection=1, eccentricity='ev',subj_id=floor(trial_id/100))%>%
    dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                  eccentricity,evidence,decision,confidence, correct)) %>%
  rbind(read_csv('../simulations/poisson_noise/discrimination.csv') %>%
  mutate(detection=0, eccentricity='fr',subj_id=floor(trial_id/100),signal=1)%>%
    dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                  eccentricity,evidence,decision,confidence, correct))%>%
  rbind(read_csv('../simulations/poisson_noise/detection.csv') %>%
  mutate(detection=1, eccentricity='fr',subj_id=floor(trial_id/100))%>%
    dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                  eccentricity,evidence,decision,confidence, correct)) %>%
  rbind(read_csv('../simulations/random_attention/discrimination.csv') %>%
  mutate(detection=0, eccentricity='ra',subj_id=floor(trial_id/100),signal=1)%>%
    dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                  eccentricity,evidence,decision,confidence, correct))%>%
  rbind(read_csv('../simulations/random_attention/detection.csv') %>%
  mutate(detection=1, eccentricity='ra',subj_id=floor(trial_id/100))%>%
    dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                  eccentricity,evidence,decision,confidence, correct)) %>%
  rbind(read_csv('../simulations/goal_directed_attention/discrimination.csv') %>%
  mutate(detection=0, eccentricity='gda',subj_id=floor(trial_id/100),signal=1)%>%
    dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                  eccentricity,evidence,decision,confidence, correct))%>%
  rbind(read_csv('../simulations/goal_directed_attention/detection.csv') %>%
  mutate(detection=1, eccentricity='gda',subj_id=floor(trial_id/100))%>%
    dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                  eccentricity,evidence,decision,confidence, correct)) %>%
  mutate(eccentricity=factor(eccentricity, levels=c('ev','fr','ra','gda')))

sim$demeaned_df <- sim$df %>%
  mutate(evidence=ifelse(signal==1 & side==bright_side,
                evidence-0.5,
                evidence),
         time=(timepoint-1));

sim$discRCdf <- sim$demeaned_df %>%
  filter(detection==0) %>%
  mutate(obj_side=factor(ifelse(side==bright_side,
                         'true',
                         'opposite'),levels=c('true','opposite')),
         side = factor(ifelse(decision==side,
                       'chosen',
                       'unchosen'),
                       levels=c('chosen','unchosen'))) %>%
  dplyr::select(subj_id,timepoint,obj_side,side,eccentricity,confidence, trial_id,evidence, time, correct) %>%
  group_by(subj_id,eccentricity) %>%
  mutate(median_confidence=median(confidence)) %>%
  ungroup() %>%
  mutate(
    binaryconf = ifelse(confidence>=median_confidence, 1, 0),
  );

sim$signalRCdf <- sim$demeaned_df %>%
  filter(detection==1 & signal==1) %>%
  mutate(side=factor(ifelse(side==bright_side,
                         'true',
                         'opposite'),levels=c('true','opposite')),
         response=decision) %>%
  dplyr::select(subj_id,timepoint,side,eccentricity,confidence, trial_id,evidence, time, correct,response) %>%
  group_by(subj_id,response,eccentricity) %>%
  mutate(median_confidence=median(confidence)) %>%
  ungroup() %>%
  mutate(
    binaryconf = ifelse(confidence>=median_confidence, 1, 0)
  );

sim <- sim %>% 
  getDiscriminationKernels() %>%
  contrastDiscriminationKernels()

sim$discrimination_accuracy_kernel %>%
  group_by(eccentricity,subj_id,contrast) %>%
  summarise(evidence=mean(evidence)) %>%
  group_by(eccentricity,contrast) %>%
  summarise(mean_evidence=mean(evidence),
            se_evidence=se(evidence)) %>%
  ggplot(aes(x=eccentricity,y=mean_evidence,fill=contrast, 
             shape=eccentricity)) +
  geom_abline(intercept=0, slope=0,size=1) +
  geom_errorbar(aes(ymin=mean_evidence-se_evidence,
                    ymax=mean_evidence+se_evidence), 
                color='black')+
  geom_point(size=6) +
  scale_shape_manual(values=c(21,22,23,24))+
  scale_color_manual(values=evidence_colors)+
  scale_fill_manual(values=evidence_colors) +
  scale_y_continuous(limits=c(-0.4,1), breaks=seq(-2,2,0.4)) +
  theme_minimal()+theme(
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank()) +
  labs(y='contrast',
       x='')+
  theme(legend.position = 'none');
  
  ggsave(paste('./figures/RC/sim/discrimination_accuracy.png',sep='/'),
         width=3.5,height=3)

sim$discrimination_confidence_kernel%>%
  group_by(side,eccentricity, subj_id) %>%
  summarise(evidence=mean(diff)) %>%
  group_by(side,eccentricity)%>%
  summarise(se=se(evidence),
            evidence=mean(evidence)) %>%
  ggplot(aes(x=eccentricity,y=evidence,fill=side,shape=eccentricity)) +
    geom_hline(yintercept=0)  +
  geom_abline(intercept=0, slope=0,size=1) +
  geom_errorbar(aes(ymin=evidence-se,
                    ymax=evidence+se), 
                color='black')+
  geom_point(size=6) +
  scale_shape_manual(values=c(21,22,23,24))+
  scale_color_manual(values=discrimination_colors)+
  scale_fill_manual(values=discrimination_colors) +
  scale_y_continuous(limits=c(-0.2,0.5)) +
  theme_minimal()+theme(
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank()) +
  labs(y='evidence',
       x='')+
  theme(legend.position = 'none');

  ggsave(paste('./figures/RC/sim/discrimination_confidence.png',sep='/'),
         width=3.5,height=3)

  sim$discrimination_confidence_kernel %>% 
    group_by(eccentricity, subj_id) %>%
    summarise(relative_evidence=mean(diff[side=='chosen'])-mean(diff[side=='unchosen']),
              sum_evidence = mean(diff[side=='chosen'])+mean(diff[side=='unchosen'])) %>%
    pivot_longer(cols=ends_with('evidence'), names_to='contrast', values_to='evidence') %>%
    group_by(contrast,eccentricity) %>%
    summarise(se=se(evidence),
              evidence=mean(evidence)) %>%
    ggplot(aes(x=eccentricity,y=evidence,fill=contrast, shape=eccentricity)) +
    geom_abline(intercept=0, slope=0,size=1) +
    geom_errorbar(aes(ymin=evidence-se,
                      ymax=evidence+se), 
                  color='black')+
    geom_point(size=6) +
    scale_shape_manual(values=c(21,22,23,24))+
    scale_color_manual(values=evidence_colors)+
    scale_fill_manual(values=evidence_colors) +
    scale_y_continuous(limits=c(-0.2,0.5), breaks=seq(-2,2,0.4)) +
    theme_minimal()+theme(
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()) +
    labs(y='contrast',
         x='')+
    theme(legend.position = 'none');

  ggsave(paste('./figures/RC/sim/discrimination_confidence_sum_rel.png',sep='/'),
         width=3.5,height=3)
  
  sim <- sim %>% 
  getDetectionSignalKernels() %>%
  contrastDetectionSignalKernels()
  
  sim$signal_decision_kernel %>%
    group_by(side,eccentricity, subj_id) %>%
    summarise(evidence = mean(evidence)) %>%
    group_by(side,eccentricity) %>%
    summarise(se=se(evidence),
              evidence=mean(evidence)) %>%
  ggplot(aes(x=eccentricity,y=evidence,fill=side,shape=eccentricity))+
  geom_abline(intercept=0, slope=0,size=1) +
  geom_errorbar(aes(ymin=evidence-se,
                    ymax=evidence+se), 
                color='black')+
  geom_point(size=6) +
  scale_shape_manual(values=c(21,22,23,24))+
  scale_color_manual(values=detection_colors)+
  scale_fill_manual(values=detection_colors) +
  scale_y_continuous(limits=c(-0.56,1.4), breaks=seq(-2,2,0.4)) +
  theme_minimal()+theme(
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank()) +
  labs(y='evidence',
       x='')+
  theme(legend.position = 'none');
  
  ggsave(paste('./figures/RC/sim/detection_decision.png',sep='/'),
         width=3.5,height=3)
  
  sim$signal_decision_kernel %>%
    group_by(subj_id,side,eccentricity) %>%
    summarise(evidence=mean(evidence)) %>%
    group_by(subj_id,eccentricity)%>%
    summarise(rel_evidence = evidence[side=='true']-evidence[side=='opposite'],
              sum_evidence = evidence[side=='true']+evidence[side=='opposite']) %>%
    pivot_longer(cols = ends_with('evidence'),names_to='contrast',values_to='evidence')%>%
    group_by(contrast,eccentricity) %>%
    summarise(se=se(evidence),
              evidence=mean(evidence)) %>%
    ggplot(aes(x=eccentricity,y=evidence,fill=contrast, shape=eccentricity)) +
    geom_abline(intercept=0, slope=0,size=1) +
    geom_errorbar(aes(ymin=evidence-se,
                      ymax=evidence+se), 
                  color='black')+
    geom_point(size=6) +
    scale_shape_manual(values=c(21,22,23,24))+
    scale_color_manual(values=evidence_colors)+
    scale_fill_manual(values=evidence_colors) +
    scale_y_continuous(limits=c(-0.56,1.4), breaks=seq(-2,2,0.4)) +
    theme_minimal()+theme(
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()) +
    labs(y='evidence',
         x='')+
    theme(legend.position = 'none');
  


  ggsave(paste('./figures/RC/sim/detection_decision_sum_rel.png',sep='/'),
         width=3.5,height=3);
  
  sim$signal_confidence_kernel %>%
    filter(response==1)%>%
    group_by(subj_id,side,eccentricity) %>%
    summarise(evidence=mean(diff)) %>%
    group_by(side,eccentricity) %>%
    summarise(se=se(evidence),
              evidence=mean(evidence))%>%
  ggplot(aes(x=eccentricity,y=evidence,fill=side,shape=eccentricity))+
  geom_abline(intercept=0, slope=0,size=1) +
  geom_errorbar(aes(ymin=evidence-se,
                    ymax=evidence+se), 
                color='black')+
  geom_point(size=6) +
  scale_shape_manual(values=c(21,22,23,24))+
  scale_color_manual(values=detection_colors)+
  scale_fill_manual(values=detection_colors) +
  scale_y_continuous(limits=c(-0.2,0.5), breaks=seq(-2,2,0.4)) +
  theme_minimal()+theme(
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank()) +
  labs(y='evidence',
       x='')+
  theme(legend.position = 'none');
  

  ggsave(paste('./figures/RC/sim/detection_conf_yes.png',sep='/'),
         width=3.5,height=3)
  
  sim$signal_confidence_kernel%>%
    filter(response==1)%>%
    group_by(subj_id,eccentricity) %>%
    summarise(rel_evidence=mean(diff[side=='true'])-mean(diff[side=='opposite']),
              sum_evidence = mean(diff[side=='true'])+mean(diff[side=='opposite'])) %>%
      pivot_longer(cols = ends_with('evidence'),names_to='contrast',values_to='evidence')%>%
      group_by(contrast,eccentricity) %>%
      summarise(se=se(evidence),
                evidence=mean(evidence)) %>%
    ggplot(aes(x=eccentricity,y=evidence,fill=contrast, shape=eccentricity)) +
    geom_abline(intercept=0, slope=0,size=1) +
    geom_errorbar(aes(ymin=evidence-se,
                      ymax=evidence+se), 
                  color='black')+
    geom_point(size=6) +
    scale_shape_manual(values=c(21,22,23,24))+
    scale_color_manual(values=evidence_colors)+
    scale_fill_manual(values=evidence_colors) +
    scale_y_continuous(limits=c(-0.3,0.75), breaks=seq(-2,2,0.4)) +
    theme_minimal()+theme(
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()) +
    labs(y='evidence',
         x='')+
    theme(legend.position = 'none');
  

  ggsave(paste('./figures/RC/sim/detection_conf_yes_sum_rel.png',sep='/'),
         width=3.5,height=3)
  
    sim$signal_confidence_kernel %>%
    filter(response==0)%>%
    group_by(subj_id,side,eccentricity) %>%
    summarise(evidence=mean(diff)) %>%
    group_by(side,eccentricity, .drop=F) %>%
    summarise(se=se(evidence),
              evidence=mean(evidence))%>%
    mutate(evidence=ifelse(is.na(se),NaN,evidence)) %>%  # to avoid plotting means of one row only
  ggplot(aes(x=eccentricity,y=evidence,fill=side,shape=eccentricity))+
  geom_abline(intercept=0, slope=0,size=1) +
  geom_errorbar(aes(ymin=evidence-se,
                    ymax=evidence+se), 
                color='black')+
  geom_point(size=6) +
  scale_shape_manual(values=c(21,22,23,24))+
  scale_color_manual(values=detection_colors)+
  scale_fill_manual(values=detection_colors) +
  scale_y_continuous(limits=c(-0.2,0.5), breaks=seq(-2,2,0.4)) +
  theme_minimal()+theme(
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank()) +
  labs(y='evidence',
       x='')+
  theme(legend.position = 'none');
  

  ggsave(paste('./figures/RC/sim/detection_conf_no.png',sep='/'),
         width=3.5,height=3)
  
  sim$signal_confidence_kernel%>%
    filter(response==0)%>%
    group_by(subj_id,eccentricity) %>%
    summarise(rel_evidence=mean(diff[side=='true'])-mean(diff[side=='opposite']),
              sum_evidence = mean(diff[side=='true'])+mean(diff[side=='opposite'])) %>%
      pivot_longer(cols = ends_with('evidence'),names_to='contrast',values_to='evidence')%>%
      group_by(contrast,eccentricity,.drop=F) %>%
      summarise(se=se(evidence),
                evidence=mean(evidence)) %>%
    mutate(evidence=ifelse(is.na(se),NaN,evidence)) %>%  # to avoid plotting means of one row only
    ggplot(aes(x=eccentricity,y=evidence,fill=contrast, shape=eccentricity)) +
    geom_abline(intercept=0, slope=0,size=1) +
    geom_errorbar(aes(ymin=evidence-se,
                      ymax=evidence+se), 
                  color='black')+
    geom_point(size=6) +
    scale_shape_manual(values=c(21,22,23,24))+
    scale_color_manual(values=evidence_colors)+
    scale_fill_manual(values=evidence_colors) +
    scale_y_continuous(limits=c(-0.2,0.5), breaks=seq(-2,2,0.4)) +
    theme_minimal()+theme(
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()) +
    labs(y='evidence',
         x='')+
    theme(legend.position = 'none');
  

  ggsave(paste('./figures/RC/sim/detection_conf_no_sum_rel.png',sep='/'),
         width=3.5,height=3)
  
  # ev = list();
  # 
  # ev$df <- sim$df %>%
  #   filter(eccentricity=='ev') %>%
  #   mutate(correct = ifelse(correct,1,0),
  #          RT=1000,
  #          logRT=log(1000),
  #          trial_number = trial_id,
  #          response=decision,
  #          conf_bi = ifelse(
  #             response==1, 
  #             as.numeric(confidence),
  #             -1*as.numeric(confidence))) %>%
  #   group_by(subj_id)%>%
  #   mutate(conf_discrete=ntile(confidence,20) %>%
  #     factor(levels=1:21))
  # 
  # ev$trial_df <- ev$df %>%
  #   filter(timepoint==1);
  # 
  # ev$detection_df <- ev$trial_df %>%
  #   filter(detection==1) %>%
  #   mutate(stimulus=signal);
  # 
  # ev$discrimination_df <- ev$trial_df %>%
  #   filter(detection==0)%>%
  #   mutate(stimulus=bright_side);
  # 
  # ev <- ev %>% 
  # generalStats2Tasks() %>%
  # testAUC() %>%
  # testzROC2tasks() %>%
  # plotAllAsymmetries('figures/RC-ev-asymmetries.pdf');
  
  
  analyzeSimTrials<- function(simulation,simulation_label) {
    
    simulation$df <- sim$df %>%
    filter(eccentricity==simulation_label) %>%
    mutate(correct = ifelse(correct,1,0),
           RT=1000,
           logRT=log(1000),
           trial_number = trial_id,
           response=decision,
           conf_bi = ifelse(
              response==1, 
              as.numeric(confidence),
              -1*as.numeric(confidence))) %>%
    group_by(subj_id)%>%
    mutate(conf_discrete=ntile(confidence,20) %>%
      factor(levels=1:21))
  
  simulation$trial_df <- simulation$df %>%
    filter(timepoint==1);
  
  simulation$detection_df <- simulation$trial_df %>%
    filter(detection==1) %>%
    mutate(stimulus=signal);
  
  simulation$discrimination_df <- simulation$trial_df %>%
    filter(detection==0)%>%
    mutate(stimulus=bright_side);
  
  simulation <- simulation %>% 
  generalStats2Tasks() #%>%
  # testAUC() %>%
  # testzROC2tasks();
  
 #simulation%>%plotAllAsymmetries(paste('figures/RC-',simulation_label,'-ev-asymmetries.pdf',sep=''));

  simulation$demeaned_df <- simulation$df %>%
  mutate(evidence=ifelse(signal==1 & side==bright_side,
                evidence-0.5,
                evidence),
         time=(timepoint-1));

  simulation$discRCdf <- simulation$demeaned_df %>%
    filter(detection==0) %>%
    mutate(obj_side=factor(ifelse(side==bright_side,
                           'true',
                           'opposite'),levels=c('true','opposite')),
           side = factor(ifelse(decision==side,
                         'chosen',
                         'unchosen'),
                         levels=c('chosen','unchosen'))) %>%
    dplyr::select(subj_id,timepoint,obj_side,side,eccentricity,confidence, trial_id,evidence, time, correct) %>%
    group_by(subj_id) %>%
    mutate(median_confidence=median(confidence)) %>%
    ungroup() %>%
    mutate(
      binaryconf = ifelse(confidence>=median_confidence, 1, 0),
    );

  simulation$signalRCdf <- simulation$demeaned_df %>%
    filter(detection==1 & signal==1) %>%
    mutate(side=factor(ifelse(side==bright_side,
                           'true',
                           'opposite'),levels=c('true','opposite')),
           response=decision) %>%
    dplyr::select(subj_id,timepoint,side,eccentricity,confidence, trial_id,evidence, time, correct,response) %>%
    group_by(subj_id,response) %>%
    mutate(median_confidence=median(confidence)) %>%
    ungroup() %>%
    mutate(
      binaryconf = ifelse(confidence>=median_confidence, 1, 0)
    );
  
  simulation <- simulation %>% 
    getDiscriminationKernels() %>%
    contrastDiscriminationKernels() %>%
    getDetectionSignalKernels() %>%
    contrastDetectionSignalKernels() %>% 
    plotDiscriminationKernels(simulation_label,c(0,13),c(-0.4,0.5),'') %>%
    plotDetectionSignalKernels(simulation_label, c(0,13),c(-0.4,0.5),'')

  
  return(simulation)
  } 
  
  ev <- list() %>%
    analyzeSimTrials('ev');
  
 fr <- list() %>%
    analyzeSimTrials('fr') %>%
    plotDiscriminationKernels('fr',c(0,13),c(-1.2,1.5),'') %>%
    plotDetectionSignalKernels('fr', c(0,13),c(-3.3,4),'');
  
 ra <- list() %>%
  analyzeSimTrials('ra');
  
 gda <- list() %>%
  analyzeSimTrials('gda');
```

# 1. Simulations

We simulated 20,000 discrimination and 20,000 detection trials per model (100 trials x 200 simulated agents per model). On each discrimination trial, the signal channel was designated as right or left with equal probability. On half of the detection trials both channels were noise channels. We then sampled, for each trial, 12 values from each channel. These 24 values were then passed on to the simulated agent, who returned a decision and a confidence rating. We then subjected the agents’ decisions and confidence ratings to a reverse correlation analysis. We now turn to describe this analysis, which will also be used to analyze the behaviour of human participants in Exp. 1-4.

## 1. Reverse correlation analysis

Following @zylberberg2012construction, we took a reverse correlation approach and asked which sources of evidence (positive, negative, relative, and sum evidence) contribute to agents’ decisions and confidence ratings. This analysis focuses on random fluctuations in signal intensity, and asks how they affect behaviour (here, decisions and confidence in these decisions). Accordingly, in analyzing data from our simulated agents, we contrasted external stimulus energy ($E$) and not internal stimulus energy ($E’$) leaving internal noise hidden.  

### 1.1.1 Methodological note: positive evidence bias in perceptual decisions

The positive evidence bias in decision confidence is often seen as particularly striking, given that positive and negative evidence are equally weighted in forming a decision [@zylberberg2012construction; @peters2017perceptual]. For example, using reverse correlation, @zylberberg2012construction showed that momentary fluctuations in the availability of perceptual evidence for and against a decision were equally predictive of the decision itself. Similarly, @peters2017perceptual showed that in classifying rapidly presented images as ‘face’ or ‘house’, decisions are not solely guided by positive evidence (e.g., face-related brain activity when deciding ‘face’), but also by negative evidence (e.g., house-related brain activity when deciding ‘face’).

In both cases, it is useful to ask what it would look like for an agent to only consider positive evidence in making a decision. This soon becomes circular, because positive and negative evidence are defined with respect to the decision itself. For example, when analyzing the decisions of an agent that consistently ignores evidence for one alternative (similar to the random attention model above), both positive and negative evidence should still be predictive of decisions. The effect of positive evidence is then driven by those trials in which the agent selected the attended alternative, and the effect of negative evidence by those trials in which the agent selected the ignored alternative (because the evidence for the attended alternative was insufficient). Put differently, asymmetries of positive and negative evidence cannot affect the decision itself, because at the time of making the decision there is no positive and negative evidence to speak of -- instead there are two sources of evidence that may become positive or negative, depending on the decision that is selected. For this reason, in measuring evidence weighting in decision formation, we defined relative and sum evidence relative to the ground truth rather than the agents’ decision.


### 1.1.2 Discrimination decisions

From each trial $(tr)$ we extracted random fluctuations in perceptual evidence in the signal $E^{tr}_s(t)$ and non-signal $E^{tr}_{n}(t)$ sensory channels. To make sure we are measuring true random fluctuations and not systematic differences between noise and signal channels, we mean centered the signal channels across trials to 0, such that the average timecourse across all agents and trials was constant at 0. For simplicity, in extracting qualitative predictions from model simulations we averaged all timepoints in a trial to obtain trial-level estimates $E^{tr}_s$ and $E^{tr}_{n}$. Human data were analyzed in a similar fashion, but separately for each timepoint. Time-resolved decision and confidence kernels derived from model simulations are available in the Appendix. 

‘Relative evidence’ was defined as the difference in noise terms between the signal and non-signal channels ($E^{tr}_{relative}=E^{tr}_s-E^{tr}_{n}$). To obtain a decision kernel, we took the difference between the average relative evidence in trials where agents chose the signal and non-signal channels $E_{relative}=\langle E^{tr}_{relative} \rangle_{CORRECT}-\langle E^{tr}_{relative} \rangle_{INCORRECT}$. This was done separately for each simulated agent, and the resulting values were tested against zero in a t-test. In all four models, relative evidence was higher on trials in which the agent correctly identified the signal channel (Fig. \@ref(fig:RC-simulations)A, orange markers). 

‘Sum evidence’ was defined as the total sum of noise terms across both channels ($E^{tr}_{sum}=E^{tr}_s+E^{tr}_{n}$). Similarly, we used the difference between sum evidence in correct and incorrect trials $E_{sum}=\langle E^{tr}_{sum} \rangle_{CORRECT}-\langle E^{tr}_{sum} \rangle_{INCORRECT}$ to probe effects of sum evidence on decision. Sum evidence had no effect on decision in any of the four models (Fig. \@ref(fig:RC-simulations)A, black markers). 

(ref:RC-simulations) Simulated predictions for the reverse correlation analysis, derived from the four models. A: effects of relative (orange markers) and sum (black markers) evidence on discrimination decisions. B: effects of evidence for the chosen (green markers) and unchosen (purple markers) alternatives on discrimination confidence. C: effects of sum and relative evidence (defined with respect to participants’ decisions) on discrimination confidence. Panels D, F and H: effects of evidence in the signal channel (blue markers) and in the non-signal channel (red markers) on detection decisions, confidence in yes responses, and confidence in no responses, respectively. Panels E, G, and I: effects of relative evidence (orange markers) and sum evidence (black markers) on detection decisions, confidence in yes responses, and confidence in no responses, respectively. For scale, grid lines are plotted in common arbitrary units. Stars represent significance in a two-sided t-test: \*: p<0.05, \*\*: p<0.01, \*\*\*: p<0.001.

```{r RC-simulations, fig.cap="(ref:RC-simulations)"}

knitr::include_graphics('figures/RC/sim/summary2.png')

```

### 1.1.3 Discrimination confidence

In all four models, confidence was defined as the Bayesian probability of being correct, given an equal prior over the two world states (see Appendix). The median confidence rating was used to split evidence channels into four sets, according to decision (chosen or unchosen, depending on the agent’s decision) and confidence level (high or low). Confidence kernels for the chosen and unchosen channels were then extracted by subtracting the mean low-confidence from the mean high-confidence values for each channel:

$$E_{conf-chosen}=\langle E^{tr}_{chosen} \rangle_{HIGH}-\langle E^{tr}_{chosen} \rangle_{LOW}$$

$$E_{conf-unchosen}=\langle E^{tr}_{unchosen} \rangle_{HIGH}-\langle E^{tr}_{unchosen} \rangle_{LOW}$$

Confidence kernels were also extracted for relative and sum evidence:

$$E_{conf-relative}=(\langle E^{tr}_{chosen} \rangle_{HIGH}-\langle E^{tr}_{unchosen} \rangle_{HIGH})-(\langle E^{tr}_{chosen} \rangle_{LOW}-\langle E^{tr}_{unchosen} \rangle_{LOW})$$

$$E_{conf-sum}=(\langle E^{tr}_{chosen} \rangle_{HIGH}+\langle E^{tr}_{unchosen} \rangle_{HIGH})-(\langle E^{tr}_{chosen} \rangle_{LOW}+\langle E^{tr}_{unchosen} \rangle_{LOW})$$

In all four models, high confidence ratings were associated with stronger evidence in the chosen channel (Fig. \@ref(fig:RC-simulations)B, green markers) and weaker evidence in the unchosen channel (Fig. \@ref(fig:RC-simulations)B, purple markers). As expected, this translated to an effect of relative evidence on decision confidence: agents were more confident when the evidence difference between the chosen and unchosen channels ($E_{conf-relative}$) was high (Fig. \@ref(fig:RC-simulations)C, orange markers).

Critically, only the firing rate and goal-directed attention models produced an effect of sum evidence ($E_{conf-sum}$) on decision confidence, such that agents were more confident when overall evidence was high (Fig. \@ref(fig:RC-simulations)C, black markers). As reviewed above, this effect is consistent with a positive evidence bias in discrimination confidence.

### 1.1.4 Detection decisions

For the reverse correlation analysis of detection decisions, we focused on trials in which a signal was present. This allowed us to disentangle the effects of evidence in the signal and non-signal channels on detection decisions and confidence. We subtracted evidence in trials that resulted in a ‘no’ (target absent) decision from evidence in trials that resulted in a ‘yes’ (target present) decision, separately for the signal and non-signal channels:

$$E_{detection-s}=\langle E^{tr}_{s} \rangle_{YES}-\langle E^{tr}_{s} \rangle_{NO}$$

$$E_{detection-n}=\langle E^{tr}_{n} \rangle_{YES}-\langle E^{tr}_{n} \rangle_{NO}$$

We similarly obtained detection kernels as a function of relative and sum evidence:

$$E_{detection-relative}=(\langle E^{tr}_{s} \rangle_{YES}-\langle E^{tr}_{n} \rangle_{YES})-(\langle E^{tr}_{s} \rangle_{NO}-\langle E^{tr}_{n} \rangle_{NO})$$

$$E_{detection-sum}=(\langle E^{tr}_{s} \rangle_{YES}+\langle E^{tr}_{n} \rangle_{YES})-(\langle E^{tr}_{s} \rangle_{NO}+\langle E^{tr}_{n} \rangle_{NO})$$

In all four models, 'yes’ responses were associated with stronger evidence in the signal channel (Fig. \@ref(fig:RC-simulations)D, blue markers). Importantly, the same was true for evidence in the non-signal channel: agents were more likely to respond ‘yes’ when evidence was stronger in this channel too (Fig. \@ref(fig:RC-simulations)D, red markers). This is a key prediction of our Bayes-rational models: in detection, evidence in both channels should be weighted positively, as the agent's goal is to detect any signal relative to noise. Together, these two positive effects translated to a strong effect of sum evidence on detection decisions: agents were more likely to respond ‘yes’ when the total sum of evidence was high (Fig. \@ref(fig:RC-simulations)E, black markers). A weaker effect of relative evidence on detection decisions was observed in all models except for the random attention model (Fig. \@ref(fig:RC-simulations)E, orange markers). 

### 1.1.5 Detection confidence

Similar to the discrimination task, the median confidence rating was used to split evidence channels into four sets, according to signal (signal channel or non-signal channel) and confidence level (high or low). This was done separately for ‘yes’ and ‘no’ responses. Confidence kernels for the signal and non-signal channels were then extracted by subtracting the mean low-confidence from the mean high-confidence evidence values for each channel and decision. For example, for 'yes' responses this meant computing:

$$E_{conf-yes-s}=\langle E^{tr}_{s} \rangle_{YES,HIGH}-\langle E^{tr}_{s} \rangle_{YES,LOW}$$
$$E_{conf-yes-n}=\langle E^{tr}_{n} \rangle_{YES,HIGH}-\langle E^{tr}_{n} \rangle_{YES,LOW}$$
$$E_{conf-yes-relative}=(\langle E^{tr}_{s} \rangle_{YES,HIGH}-\langle E^{tr}_{n} \rangle_{YES,HIGH})- (\langle E^{tr}_{s} \rangle_{YES,LOW}-\langle E^{tr}_{n} \rangle_{YES,LOW})$$
$$E_{conf-yes-sum}=(\langle E^{tr}_{s} \rangle_{YES,HIGH}+\langle E^{tr}_{n} \rangle_{YES,HIGH})- (\langle E^{tr}_{s} \rangle_{YES,LOW}+\langle E^{tr}_{n} \rangle_{YES,LOW})$$

In all four models, agents were more confident in their decisions about signal presence when evidence in the signal channel was stronger (Fig. \@ref(fig:RC-simulations)F, blue markers). Mirroring the detection decision kernel means, confidence in signal presence was also positively affected by evidence for signal in the non-signal channel (Fig. \@ref(fig:RC-simulations)F, red markers). Together, these two positive effects produced an overall positive effect of sum evidence on confidence in signal presence (Fig. \@ref(fig:RC-simulations)G, black markers). All four models predicted a weaker effect of relative evidence (Fig. \@ref(fig:RC-simulations)G, orange markers).

Finally, we asked how random variability in sensory noise contributed to confidence in detection “no” responses. Here, a low number of misses made it difficult to reliably estimate confidence kernels for the firing rate model. In the remaining three models, agents were more confident in decisions about signal absence when evidence in both signal and non-signal channels was weaker (Fig. \@ref(fig:RC-simulations)H, blue and red markers, respectively). Together, these negative effects translated to a total negative effect of sum evidence on confidence in absence (Fig. \@ref(fig:RC-simulations)I, black markers). None of the four models predicted a negative effect of relative evidence on confidence in absence, but the random attention model predicted a subtle positive effect (Fig. \@ref(fig:RC-simulations)I, orange markers).

Equipped with qualitative predictions from four Bayes-rational models, we now turn to describing our empirical results. As we report below, these models failed to account for a key signature of human decision making: in both decisions and confidence ratings, subjects negatively weigh evidence in the non-signal channel when inferring signal presence, as if they are making a discrimination judgment about the origin of the signal, rather inferring signal presence. 

# 2. Experiment 1

## 2.1 Methods

### 2.1.1 Participants

The research complied with all relevant ethical regulations, and was approved by the Research Ethics Committee of University College London (study ID number 1260/003). 10 participants were recruited via the UCL’s psychology subject pool, and gave their informed consent prior to their participation. Each participant performed four sessions of 600 trials each, in blocks of 100 trials. Sessions took place on different days and consisted of 3 discrimination blocks interleaved with 3 detection blocks.

### 2.1.2 Experimental procedure

The experimental procedure for Exp. 1 largely followed the procedure described in @zylberberg2012construction, Exp. 1. Participants observed a random-dot kinematogram for a fixed duration of 700 ms. In discrimination trials, the direction of motion was one of two opposite directions with equal probability, and participants reported the observed direction by pressing one of two arrow keys on a standard keyboard. In detection blocks participants reported whether there was any coherent motion by pressing one of two arrow keys on a standard keyboard. In half of the detection trials dots moved coherently to one of two opposite directions, and in the other half all dots moved randomly. 

In both detection and discrimination blocks, participants indicated their confidence following each decision. Confidence was reported on a continuous scale ranging from chance to complete certainty. To avoid systematic response biases affecting confidence reports, the orientation (vertical or horizontal) and polarity (e.g., right or left) of the scale was set to agree with the type 1 response. For example, following an up arrow press, a vertical confidence bar was presented where ‘guess’ is at the center of the screen and ‘certain’ appeared at the upper end of the scale (see Fig. \@ref(fig:RC-design1)).

(ref:RC-design1) Task design for Experiment 1. In both discrimination and detection blocks, participants viewed 700 milliseconds of a random dot motion array, after which they made a keyboard response to indicate their decision (motion direction in discrimination, signal absence or presence in detection), followed by a continuous confidence report using the mouse. 5 participants viewed vertically moving dots and indicated their detection responses on a horizontal scale, and 5 participants viewed horizontally moving dots and indicated their detection responses on a vertical scale.

```{r RC-design1, fig.cap="(ref:RC-design1)"}

knitr::include_graphics('figures/design1.png')

```

To control for response requirements, for five subjects the dots moved to the right or to the left, and for the other five subjects they moved upward or downward. The first group made discrimination judgments with the right and left keys and detection judgments with the up and down keys, and this mapping was reversed for the second group. The number of coherently moving dots (‘motion coherence’) was adjusted to maintain performance at around 70% accuracy for detection and discrimination tasks independently. This was achieved by measuring mean accuracy after every 20 trials, and adjusting coherence by a step of 3% if accuracy fell below 60% or went above 80%. We opted for a block-wise staircasing procedure in order to keep motion energy relatively stable across trials, allowing participants to optimally place their detection criterion. The staircasing procedure for both tasks started at a coherence value of 1.0.

Stimuli for discrimination blocks were generated using the exact same procedure reported in @zylberberg2012construction^[We reused the original Matlab code that was used for Exp. 1 in Zylberberg et. al. (2012), kindly shared by Ariel Zylberberg. ]. Trials started with a presentation of a fixation cross for one second, immediately followed by stimulus presentation. The stimulus consisted of 152 white dots (diameter = $0.14^\circ$), presented within a $6.5^\circ$ circular aperture centered on the fixation point for 700 milliseconds (42 frames, frame rate = 60 Hz). Dots were grouped in two sets of 76 dots each. Every other frame, the dots of one set were replaced with a new set of randomly positioned dots. For each coherence value of $c'$, a proportion of $c'$ of the dots from the second set moved coherently in one direction by a fixed distance of $0.33^\circ$, while the remaining dots in the set moved in random directions by a fixed distance of $0.33^\circ$. On the next update, the sets were switched, to prevent participants from tracing the position of specific dots. Frame-specific coherence values were sampled for each screen update from a normal distribution centred around the coherence value $c$ with a standard deviation of 0.07, with the constraint that $c'$ must be a number between 0 and 1.

Stimuli for detection blocks were generated using a similar procedure, where on a random half of the trials coherence was set to 0%, without random sampling of coherence values for different frames.

To probe global metacognitive estimates of task performance, at the end of each experimental block (100 trials) participants estimated the number of correct responses they have made. Analysis of these global metacognitive estimates is provided in the Appendix.

## 2.2 Analysis

Experiment 1 was pre-registered (pre-registration document is available here: [https://osf.io/z2s93/](https://osf.io/z2s93/)). Our full pre-registered analysis is available in the Appendix.

### 2.2.1 Reverse correlation analysis

For the reverse correlation analysis, we followed a procedure similar to the one described in @zylberberg2012construction. For each of the four directions (right, left, up and down), we applied two spatiotemporal filters to the frames of the dot motion stimuli as described in previous studies [@adelson1985spatiotemporal;@zylberberg2012construction]. The outputs of the two filters were squared and summed, resulting in a three-dimensional matrix with motion energy in a specific direction as a function of x, y, and time. We then took the mean of this matrix across the x and y dimensions to obtain an estimate of the overall temporal fluctuations in motion energy in the selected direction. Using this filter, we obtained estimates of temporal fluctuations in the mean and variance of motion energy for upward, downward, leftward and rightward motion within each trial. We refer to these temporal estimates as motion energy vectors, where each such vector consists of 42 entries, one per timepoint. Additionally, for every time point we extracted the variance along the x and y dimensions, but given the high correlation between our estimates of mean and variance, we focused our analysis on the mean motion energy.

In order to distill random fluctuations in motion energy from mean differences between stimulus categories, we subtracted the mean motion energy from trial-specific motion energy vectors. The mean motion energy vectors were extracted by averaging the motion energy vectors of all participants, separately for each motion coherence level and motion direction. We chose this approach instead of the linear regression approach used by @zylberberg2012construction in order to be sensitive to the possibility of nonlinear effects of coherence on motion energy.

## 2.3 Results

```{r RC-exp1-load-and-format-data, echo=FALSE, cache=TRUE, message=FALSE}

e1 = list()


e1$df <- read_csv('../experiments/Experiment1/data/RC.csv', lazy=FALSE) %>%
  group_by(subj_id, detection) %>%
  mutate(confidence=confidence/1000,
         # in the original coding, 3 is right and 1 is left. 
         # changed to be 0 for right/up and 1 for left/down, to align 
         # with the coding of responses.
         direction = ifelse(direction==3,1,0)); 
 
e1$trial_df_unfiltered <- e1$df %>%
  group_by(subj_id, trial_id) %>%
  summarise(
    detection = detection[timepoint==1],
    direction = direction[timepoint==1],
    signal = signal[timepoint==1],
    response = response[timepoint==1],
    RT = RT[timepoint==1]-700, # stimulus duration
    confidence = confidence[timepoint==1],
    correct = correct[timepoint==1],
    trial_number = trial_number[timepoint==1],
    logRT = log(RT[timepoint==1]),
    conf_bi = ifelse(
        response==1, 
        as.numeric(confidence),
        -1*as.numeric(confidence)))%>%
  group_by(subj_id) %>%
  mutate(
    conf_discrete = ntile(confidence,20) %>%
      factor(levels=1:21))

e1$task_stats_unfiltered <- e1$trial_df_unfiltered %>%
  group_by(subj_id,detection) %>%
  summarise(acc=mean(correct),
            RT = median(RT),
            confidence=mean(confidence));

e1$trial_df <- e1$trial_df_unfiltered %>%
  filter(trial_number>300);

e1$detection_df <- e1$trial_df %>%
  filter(detection==1) %>%
  mutate(stimulus=signal);

e1$discrimination_df <- e1$trial_df %>%
  filter(detection==0) %>%
  mutate(stimulus = direction);

```


```{r RC-exp1-general-stats, echo=FALSE, cache=TRUE, message=FALSE}
e1 <- e1 %>% 
  generalStats2Tasks() %>%
  testAUC() %>%
  testzROC2tasks();
```

### 2.3.1 Decision accuracy

Overall proportion correct was `r e1$task_stats_unfiltered%>%filter(detection==0)%>%pull(acc)%>%mean()%>%printnum()` in the discrimination and `r e1$task_stats_unfiltered%>%filter(detection==1)%>%pull(acc)%>%mean()%>%printnum()` in the detection task. Performance in discrimination was significantly higher than in detection (`r t.test(e1$task_stats_unfiltered%>%filter(detection==0)%>%pull(acc), e1$task_stats_unfiltered%>%filter(detection==1)%>%pull(acc), paired=TRUE)%>%apa_print()%>%'$'(full_result)`). This difference in task performance reflected a slower convergence of the staircasing procedure for the discrimination task during the first session. When discarding all data from the first session and analyzing only data from the last three sessions (1800 trials per participant), task performance was equated between the two tasks at the group level (`r t.test(e1$task_stats%>%filter(detection==0)%>%pull(acc), e1$task_stats%>%filter(detection==1)%>%pull(acc), paired=TRUE)%>%apa_print()%>%'$'(full_result)`; `r ttestBF(e1$task_stats%>%filter(detection==0)%>%pull(acc), e1$task_stats%>%filter(detection==1)%>%pull(acc), paired=TRUE)%>%apa_print()%>%'$'(statistic)`).  In order to avoid confounding differences between discrimination and detection decision and confidence profiles with more general task performance effects, the first session was excluded from all subsequent analyses.

### 2.3.2 Overall properties of response time and confidence distributions

In detection, participants were more likely to respond 'yes' than 'no' (mean proportion of 'yes' responses: `r t.test(e1$detection_stats$general$bias,mu=0.5)%>%apa_print()%>%'$'(full_result)`). We did not observe a consistent response bias for the discrimination data (mean proportion of 'rightward' or 'upward' responses: `r t.test(e1$discrimination_stats$general$bias,mu=0.5)%>%apa_print()%>%'$'(full_result)`). 

Replicating previous studies [@meuwese2014subjective; @mazor2020distinct; @kellij2021investigation; @mazor2021stage], we find the typical asymmetries between detection 'yes' and 'no' responses in response time, overall confidence, and the alignment between subjective confidence and objective accuracy (also termed metacognitive sensitivity, measured as the area under the response-conditional type 2 ROC curve). 'No' responses were slower compared to 'yes' responses (median difference: `r e1$detection_stats$contrast_responses%>%pull(RT)%>%median()%>%abs()%>%printnum()` ms), and accompanied by lower levels of subjective confidence (mean difference of `r e1$detection_stats$contrast_responses$confidence%>%mean()%>%printnum()` on a 0-1 scale). Metacognitive sensitivity was higher for detection 'yes' compared with detection 'no' responses (mean difference in area under the curve units: `r e1$AUC%>%filter(detection==1)%>%pull(metacognitive_asymmetry)%>%mean()%>%printnum()`). No difference in response time, confidence, or metacognitive sensitivity was found between the two discrimination responses. For a detailed statistical analysis of these behavioural asymmetries see Appendix.


### 2.3.3 Reverse correlation


```{r RC-exp1-discrimination-RC, echo=FALSE, cache= TRUE,  message=FALSE, results='hide', warning=FALSE, fig.cap="(ref:exp1-discrimination)", fig.scap="Reverse correlation of discrimination trials, Exp. 1", fig.show='hide'}

e1$discRCdf <- e1$df %>%
  filter(detection==0 & 
           trial_number>300) %>%
  dplyr::select(-c('energyDown','energyUp'))%>%
  pivot_longer(cols=starts_with('energy'),values_to='energy',names_to='measured_direction')%>%
  mutate(measured_direction=ifelse(measured_direction=='energyRight',0,1),
         obj_side=factor(ifelse(measured_direction==direction, 'true','opposite'),
                         levels=c('true','opposite')),
         side=ifelse(response==measured_direction,'chosen','unchosen'),
         session = round(trial_number/300)) %>%
  group_by(subj_id, session) %>%
  mutate(median_confidence=median(confidence)) %>%
  ungroup() %>%
  mutate(
    binaryconf = ifelse(confidence>=median_confidence, 1, 0),
    eccentricity=1,
    time=(timepoint-4)/60*1000)%>%
  group_by(obj_side)%>%
  mutate(evidence=energy/sd(energy))

e1 <- e1 %>% 
  getDiscriminationKernels() %>%
  contrastDiscriminationKernels()

e1$discrimination_accuracy_kernel <- e1$discrimination_accuracy_kernel %>%
  group_by(subj_id, contrast)%>%
  arrange(time) %>%
  mutate(evidence=rollapply(evidence,5,mean,align='right',fill=NA))

e1$discrimination_confidence_kernel <- e1$discrimination_confidence_kernel %>%
  group_by(subj_id, side)%>%
  arrange(time) %>%
  mutate(diff=rollapply(diff,5,mean,align='right',fill=NA));

e1 %>% plotDiscriminationKernels('exp1',c(0,750),c(-0.3,0.4), 'motion energy (a.u.)')
```

#### Discrimination

Using reverse correlation we quantified the effect of random fluctuations in motion energy on the probability of correctly identifying the true direction of motion, and on the temporal dynamics of decision formation. Importantly, this analysis approach treats leftward and rightward motion energy as two independently represented quantities, assuming that the decision-making module has access to individual spatiotemporal filters, and not only to the difference between them [@levinson1975independence; @adelson1985spatiotemporal; @van1984temporal]. We return to this point in describing the rationale for Exp. 2. 

Following @zylberberg2012construction, we focused our analysis on the first 300 ms of the trial. Participants’ discrimination responses were significantly affected by the relative evidence for the true direction of motion compared to the opposite direction ($E_{relative}$; `r apa_print(e1$RC$accuracy_rel300$diff%>%t.test())$statistic`), whereas sum evidence ($E_{sum}$; the total amount of energy in both directions) had no effect on discrimination accuracy (`r apa_print(e1$RC$accuracy_sum300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp1-RC)A). This is consistent with a symmetric weighting of evidence in decision formation, and with the predictions of all four models.

We next turned to the contribution of motion energy to subjective confidence ratings. The median confidence rating in each experimental session was used to split all motion energy vectors into four groups, according to decision (chosen or unchosen directions) and confidence level (high or low). Confidence kernels for the chosen and unchosen directions were then extracted by subtracting the mean low-confidence from the mean high-confidence vectors for both the chosen and unchosen directions. Motion energy in the chosen direction (positive evidence) significantly increased confidence ($E_{conf-chosen}$; `r apa_print(e1$RC$confidence_pos300$diff%>%t.test())$statistic`), but we found no significant decrease in confidence with stronger motion energy in the opposite direction ($E_{conf-unchosen}$; `r apa_print(e1$RC$confidence_neg300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp1-RC)B). Equivalently, both relative and sum evidence positively contributed to decision confidence ($E_{conf-relative}$: `r apa_print(e1$RC$confidence_rel300$diff%>%t.test())$statistic`; $E_{conf-sum}$: `r apa_print(e1$RC$confidence_sum300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp1-RC)C). This is a replication of the positive evidence bias observed in @zylberberg2012construction, and consistent with the predictions of the firing rate and goal-directed attention models.

```{r RC-exp1-signal-RC, echo=FALSE, cache= TRUE, message=FALSE, warning=FALSE, results='hide', fig.cap="(ref:exp1-discrimination)", fig.scap="Reverse correlation of discrimination trials, Exp. 1", fig.show='hide'}

e1$signalRCdf <- e1$df %>%
  filter(detection==1 & 
           signal==1 &
           trial_number>300) %>%
  dplyr::select(-c('energyDown','energyUp'))%>%
  pivot_longer(cols=starts_with('energy'),values_to='energy',names_to='measured_direction')%>%
  mutate(measured_direction=ifelse(measured_direction=='energyRight',0,1),
         side=factor(ifelse(measured_direction==direction, 'true','opposite'),
                     levels=c('true','opposite')),
         session = round(trial_number/300)) %>%
  group_by(subj_id, session,response) %>%
  mutate(median_confidence=median(confidence)) %>%
  ungroup() %>%
  mutate(
    binaryconf = ifelse(confidence>=median_confidence, 1, 0),
    eccentricity=1,
    time=(timepoint-4)/60*1000)%>%
  group_by(side)%>%
  mutate(evidence=energy/sd(energy))

e1 <- e1 %>% 
  getDetectionSignalKernels() %>%
  contrastDetectionSignalKernels()

e1$signal_decision_kernel <- e1$signal_decision_kernel %>%
  group_by(subj_id,side)%>%
  arrange(time) %>%
  mutate(evidence=rollapply(evidence,5,mean,align='right',fill=NA))

e1$signal_confidence_kernel <- e1$signal_confidence_kernel %>%
  group_by(subj_id, side,response)%>%
  arrange(time) %>%
  mutate(diff=rollapply(diff,5,mean,align='right',fill=NA));

e1 %>% plotDetectionSignalKernels('exp1',c(0,750),c(-0.3,0.4), 'motion energy (a.u.)')
```

(ref:RC-exp1-RC) Reverse correlation, Exp. 1. A: effects of relative (black curve) and sum (orange curve) evidence on discrimination decisions. Note that relative evidence here is defined with respect to the true direction of motion, not participants’ decisions. B: effects of evidence for the chosen (green curve) and unchosen (purple curve) alternative on discrimination confidence. C: effects of sum and relative evidence (defined with respect to participants’ decisions) on discrimination confidence. Panels D, F and H: effects of evidence for the true direction of motion (blue curve) and for the opposite direction of motion (red curve) on detection decisions, confidence in yes responses, and confidence in no responses, respectively. Panels E, G, and I: effects of relative evidence (black curve) and sum evidence (orange curve) on detection decisions, confidence in yes responses, and confidence in no responses, respectively. The first 300 milliseconds of the trial are marked in black. All nine panels are presented at the same scale, in arbitrary motion-energy units. Stars represent significance in a two-sided t-test for the first 300 milliseconds of the trial: \*: p<0.05, \*\*: p<0.01, \*\*\*: p<0.001.

```{r RC-exp1-RC, echo=FALSE, message=FALSE, fig.cap="(ref:RC-exp1-RC)"}

knitr::include_graphics("figures/RC/exp1/summary.png")
```

#### Detection

Participants were significantly more likely to respond ‘yes’ when fluctuations in motion energy during the first 300 milliseconds of the trial strengthened motion energy in the true direction of motion ($E_{detection-s}$`r apa_print(e1$RC$signal_decision_pos300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp1-RC)D, blue curve). Critically, and in contrast to the predictions of all four Bayes-rational models, motion energy in the opposite direction had a negative, rather than a positive effect on the probability of responding ‘yes’ ($E_{detection-n}$; `r apa_print(e1$RC$signal_decision_neg300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp1-RC)D, red curve). In other words, stronger motion energy in the opposite direction made it less likely that people would say a signal was present. 

Confidence ratings were higher in detection ‘yes’ responses when random noise strengthened the motion energy in the true direction of motion ($E_{conf-yes-s}$; `r apa_print(e1$RC$signal_confidenceYes_pos300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp1-RC)F, blue curve). Again, in contrast to our model predictions, motion energy in the opposite direction had a negative, rather than a positive effect on detection confidence. That is, subjects were more confident in the presence of coherent motion when there was an unusually low level of motion energy in one of the two directions ($E_{conf-yes-n}$; `r apa_print(e1$RC$signal_confidenceYes_neg300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp1-RC)F, red curve). 

Furthermore, unlike in the discrimination task, we found no effect of sum evidence on confidence ratings in 'yes' responses ($E_{conf-yes-sum}$; `r apa_print(e1$RC$signal_confidenceYes_sum300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp1-RC)G, black curve). To reiterate, while detection *decisions* were mostly sensitive to fluctuations in motion energy toward the true direction of motion, *confidence judgments* in detection ‘yes’ responses were equally sensitive (with opposite signs) to fluctuations in the true and opposite directions of motion. However, and to anticipate the results of Exp. 3 presented below, we note that this symmetric weighting of evidence in detection confidence was not replicated in a subsequent experiment designed to directly test this surprising result.

Finally, confidence in ‘no’ responses was independent of relative, sum, positive, and negative evidence (all p's>0.1; see Fig. 6H). 

# 3. Experiment 2

In Experiment 1, we replicated previous observations of a positive evidence bias in discrimination confidence, such that confidence scaled with the total sum of evidence for both hypotheses. In contrast, in detection an effect of sum evidence was apparent for the decision, but not for the confidence kernels. Furthermore, confidence in detection ‘no’ responses was unaffected by fluctuations in motion energy.

Importantly, our analysis treated energy in the leftward and rightward directions as two independently represented quantities. Although models of motion perception commonly include such direction-selective sensory channels [@levinson1975independence; @adelson1985spatiotemporal; @van1984temporal], it is unclear to what degree left and right motion energy channels are available to decision-making modules, as opposed to a mere subtraction between the two.  In Exp. 2, the two sensory channels corresponded to two separate stimuli, making it much more likely that subjects represented them in an independent manner. Using these stimuli, we tested the generalizability of these findings to a different type of stimuli (flickering patches) and mode of data collection (a ~10 minute online experiment). Our pre-registered objectives (documented here: [https://osf.io/d3vkm/](https://osf.io/d3vkm/)) were 1) to replicate a positive evidence bias in discrimination confidence, 2) to replicate the absence of a positive evidence bias in detection confidence, 3) to replicate the absence of an effect of evidence on confidence in ‘no’ judgments.

## 3.1 Methods

### 3.1.2 Participants

The research complied with all relevant ethical regulations, and was approved by the Research Ethics Committee of University College London (study ID number 1260/003). 147 participants were recruited via Prolific (prolific.co) and gave their informed consent prior to their participation. They were selected based on their acceptance rate (>95%) and for being native English speakers. Following our pre-registration, we aimed to collect data until we had reached 100 included participants based on our pre-specified inclusion criteria (see [https://osf.io/d3vkm/](https://osf.io/d3vkm/)). Our final data set includes observations from 102 included participants. The entire experiment took around 10 minutes to complete. Participants were paid £1.25 for their participation, equivalent to an hourly wage of £7.50.


### 3.1.3 Experimental paradigm

The experiment was programmed using the jsPsych and P5 JavaScript packages [@de2015jspsych; @mccarthy2015p5], and was hosted on a JATOS server [@lange2015jatos]. It consisted of two tasks (Detection and Discrimination) presented in separate blocks. A total of 56 trials of each task was delivered in 2 blocks of 28 trials each. The order of experimental blocks was interleaved, starting with discrimination.

The first discrimination block started after an instruction section, which included instructions about the stimuli and confidence scale, four practice trials and four confidence practice trials. Further instructions were presented before the second block. Instruction sections were followed by multiple-choice comprehension questions, to monitor participants’ understanding of the main task and confidence reporting interface. To encourage concentration, in addition to trial-wise feedback we also provided participants with feedback about their overall performance and mean confidence at the end of the second and fourth blocks.

Importantly, unlike in the lab-based experiment, there was no calibration of difficulty for the two tasks. The rationale for this is that in Exp. 1 perceptual thresholds for motion discrimination were highly consistent across participants, and staircasing took a long time to converge. Furthermore, in Exp. 1 we aimed to control for task difficulty, but this introduced differences between the stimulus intensities used for detection and discrimination. To complement our findings, here we aimed to match stimulus intensity between the two tasks, and accepted that task performance might vary between detection and discrimination as a result.

### 3.1.4 Trial structure

In discrimination blocks, trial structure closely followed Exp. 2 from @zylberberg2012construction, with a few adaptations. Following a fixation cross (500 ms), two sets of four adjacent vertical gray bars were presented as a rapid serial visual presentation (RSVP; 12 frames, presented at 25Hz), displayed to the left and right of the fixation cross (see Fig. 7). On each frame, the luminance of each bar was randomly sampled from a Gaussian distribution with a standard deviation of 10/255 units in the standard RGB 0-255 coordinate system. For one set of bars, this Gaussian distribution was centered at the same luminance value as the background (128/255). For the other set, it was centered at 133/255, making it brighter on average. Participants then reported which of the two sets was brighter on average using the ‘D’ and ‘F’ keys on the keyboard. After their response, they rated their confidence on a continuous scale, by controlling the size of a colored circle with their mouse. High confidence was mapped to a big, blue circle, and low confidence to a small, red circle. To discourage hasty confidence ratings, the confidence rating scale stayed on the screen for at least 2000 milliseconds. Feedback about decision accuracy was delivered after the confidence rating phase.

(ref:RC-design2) Task design for Experiment 2. In both tasks, participants viewed two flickering patches for 480 milliseconds, after which they made a keyboard response to indicate which of the patches was brighter (discrimination) or whether any of the patches was brighter than the background (detection).

```{r RC-design2, fig.cap="(ref:RC-design2)"}

knitr::include_graphics('figures/design2.png')

```

Detection blocks were similar to discrimination blocks, with the exception that decisions were made about whether the average luminance of either of the two sets was brighter than the gray background, or not. In ‘different’ trials, the luminance of the four bars in one of the sets was sampled from a Gaussian distribution with mean 133/255, and the luminance of the other set from a Gaussian distribution with mean 128/255. In ‘same’ trials, the luminance of both sets was sampled from a distribution centered at 128/255. Participants were told that only one of the two patches could be bright, but never both. Decisions in Detection trials were reported using the ‘Y’ and ‘N’ keys. Confidence ratings and feedback were as in the discrimination task.

<!-- ### 2.1.5 Randomization -->

<!-- The order and timing of experimental events was determined pseudo-randomly by the Mersenne Twister pseudorandom number generator, initialized in a way that ensures registration time-locking [@mazor2019novel].  -->


## 3.2 Results

```{r RC-exp2-load-and-format-data, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE}

e2=list()

e2$df <- read_csv('../experiments/Experiment2/data/Flicker.csv', lazy=FALSE) %>%
  group_by(s,task,trial_index) %>%
  mutate(side=c(rep(0,48),rep(1,48)),
         timepoint=rep(1:12,8), 
         eccentricity = rep(c(rep(1,12),rep(2,12),rep(3,12),rep(4,12)),2)) %>%
  rename(subj_id = s,
         trial_id = trial_index) %>%
  mutate(detection = ifelse(task=='detection',1,0),
         bright_side=ifelse(bright_side=='right',1,0),
         signal=signal_presence,
         detection=as.factor(detection),
         response=as.factor(response),
         correct=as.factor(correct),
         subj_id=as.factor(subj_id)) %>%
  ungroup() %>%
  dplyr::select(subj_id,detection,trial_id,
                RT,signal,correct,bright_side,
                response,conf_RT,confidence,
                luminance,side,timepoint,
                eccentricity,trial)


e2$trial_df <- e2$df %>%
  group_by(subj_id, trial_id) %>%
  summarise(
    detection = detection[side==1 & timepoint==1 & eccentricity == 1],
    bright_side = bright_side[side==1 & timepoint==1 & eccentricity == 1],
    signal = signal[side==1 & timepoint==1 & eccentricity == 1],
    response = response[side==1 & timepoint==1 & eccentricity == 1],
    RT = RT[side==1 & timepoint==1 & eccentricity == 1]-480, # stimulus duration
    confidence = confidence[side==1 & timepoint==1 & eccentricity == 1],
    correct = correct[side==1 & timepoint==1 & eccentricity == 1],
    trial_number = trial[side==1 & timepoint==1 & eccentricity == 1],
    conf_bi = ifelse(
      response==1, 
      as.numeric(confidence),
      -1*as.numeric(confidence)),
    luminance_0=mean(luminance[side==0 & timepoint<8]),
    luminance_1=mean(luminance[side==1 & timepoint<8]))%>%
  group_by(subj_id) %>%
  mutate(
    conf_discrete = ntile(confidence,20) %>%
      factor(levels=1:21),
    logRT=log(RT),
    evidence_0=(luminance_0-128)/5,
    evidence_1=(luminance_1-128)/5,
    stimulus=bright_side);

e2$detection_df <- e2$trial_df %>%
  filter(detection==1) %>%
  mutate(stimulus=signal);

e2$discrimination_df <- e2$trial_df %>%
  filter(detection==0) %>%
  mutate(stimulus = bright_side);
```

```{r RC-exp2-general-stats, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE}

e2 <- e2 %>% generalStats2Tasks() %>%
  testAUC() %>%
  testzROC2tasks();

e2$zROC <- rbind(e2$detection_zROC%>%mutate(detection=1),
                 e2$discrimination_zROC%>%mutate(detection=0));
e2$zROC_contrast_rsquare <- e2$zROC%>%group_by(subj_id)%>%summarize(fit_diff=r.squared[detection==1]-r.squared[detection==0])

```

### 3.2.1 Decision accuracy

Overall proportion correct was `r e2$task_stats%>%filter(detection==0)%>%pull(acc)%>%mean()%>%printnum()` in the discrimination and `r e2$task_stats%>%filter(detection==1)%>%pull(acc)%>%mean()%>%printnum()` in the detection task. Performance in discrimination was significantly higher than in detection (`r t.test(e2$task_stats%>%filter(detection==0)%>%pull(acc), e2$task_stats%>%filter(detection==1)%>%pull(acc), paired=TRUE)%>%apa_print()%>%'$'(full_result)`). Unlike in Exp. 1, where we aimed to control for task difficulty, here we decided to match stimulus intensity between the two tasks, so a difference between detection and discrimination performance was expected [@wickens2002elementary, 104]. 

### 3.2.2 Overall properties of decision and confidence distributions

Similar to Exp. 1, participants were more likely to respond 'yes' than 'no' in the detection task (mean proportion of 'yes' responses: `r mean(e2$detection_stats$general$bias)%>%printnum()`). We did not observe a consistent response bias in discrimination (mean proportion of 'right' responses: `r mean(e2$discrimination_stats$general$bias)%>%printnum()`). The two detection responses showed the typical asymmetries, with 'yes' responses being faster (median difference of `r e2$detection_stats$contrast_responses%>%pull(RT)%>%median()%>%abs()%>%round()` ms) and accompanied by higher levels of confidence (mean difference of `r e2$detection_stats$contrast_responses$confidence%>%mean()%>%printnum()` on a 0-1 scale). Unlike in Exp. 1, here we found no evidence for a difference in metacognitive sensitivity between 'yes' and 'no' responses (mean difference of `r e2$AUC%>%filter(detection==1)%>%pull(metacognitive_asymmetry)%>%mean()%>%printnum()` in AUC units). No asymmetries were observed between the two discrimination responses. For a detailed statistical analysis see Appendix.


### 3.2.3 Reverse correlation 

Stimuli in Exp. 2 consisted of two flickering patches, each comprising 4 gray bars presented for 12 frames. Together, this summed to 96 random luminance values per trial, which we subjected to reverse correlation analysis, following the analysis procedure of Exp 2. in @zylberberg2012construction.  


```{r RC-exp2-discrimination-RC, cache= TRUE, echo=FALSE, message=FALSE, results='hide', warning=FALSE, fig.show='hide'}

e2$demeaned_df <- e2$df %>%
  mutate(
    evidence = ifelse(side==bright_side & signal==1,
                      (luminance-133)/5,
                      (luminance-128)/5)
  )

e2$discRCdf <- e2$demeaned_df %>%
  filter(detection==0) %>%
  mutate(obj_side=factor(ifelse(side==bright_side, 'true','opposite'),
                         levels=c('true','opposite')),
         side=ifelse(response==side,'chosen','unchosen')) %>%
  group_by(subj_id) %>%
  mutate(median_confidence=median(confidence)) %>%
  ungroup() %>%
  mutate(
    binaryconf = ifelse(confidence>=median_confidence, 1, 0),
    time=(timepoint-1)*40) 

e2 <- e2 %>% 
  getDiscriminationKernels() %>%
  contrastDiscriminationKernels() %>%
  plotDiscriminationKernels('exp2',c(0,440),c(-0.4,0.7), 'luminance (RGB units)')
```

#### Discrimination decisions 

First, we asked whether random fluctuations in luminance influenced discrimination responses. Similar to the results obtained by Zylberberg et. al., discrimination decisions were sensitive to fluctuations in relative evidence (the difference in mean luminance between the left and right stimulus) during the first 300 milliseconds of the trial ($E_{relative}$; `r apa_print(e2$RC$accuracy_rel300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp2-RC)A, orange curve). Furthermore, participants’ decisions were surprisingly more sensitive to evidence in the non-target stimulus within the same time window, resulting in a negative effect of sum evidence ($E_{sum}$; `r apa_print(e2$RC$accuracy_sum300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp2-RC)A, black curve). Importantly, this negative effect of sum evidence on decision accuracy was not replicated in Exp. 3 and 4, and we do not interpret it further.

(ref:RC-exp2-RC) Reverse correlation, Exp. 2. Same conventions as in Fig. \@ref(fig:RC-exp1-RC).

```{r RC-exp2-RC, echo=FALSE, message=FALSE, fig.cap="(ref:RC-exp2-RC)"}

knitr::include_graphics("figures/RC/exp2/summary.png")
```

#### Discrimination confidence

Similar to Exp. 1, we observed a significant effect of positive ($E_{conf-chosen}$; `r apa_print(e2$RC$confidence_pos300$diff%>%t.test())$statistic`) and negative ($E_{conf-unchosen}$; `r apa_print(e2$RC$confidence_neg300$diff%>%t.test())$statistic`) evidence on decision confidence within the first 300 milliseconds of the stimulus (see Fig. \@ref(fig:RC-exp2-RC)B). When plotting the sum-evidence kernel, we observed an initial negative dip followed by a sustained positive effect of sum evidence on decision confidence, consistent with a positive evidence bias ($E_{conf-sum}$; `r apa_print(e2$RC$confidence_sum300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp2-RC)C, black curve).

```{r RC-exp2-signal-RC, cache= TRUE, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}

e2$signalRCdf <- e2$demeaned_df %>%
  filter(detection==1 & signal==1) %>%
  mutate(side=factor(ifelse(side==bright_side, 'true','opposite'),
                         levels=c('true','opposite'))) %>%
  group_by(subj_id,response) %>%
  mutate(median_confidence=median(confidence)) %>%
  ungroup() %>%
  mutate(
    binaryconf = ifelse(confidence>=median_confidence, 1, 0),
    time=(timepoint-1)*40) 

e2 <- e2 %>% 
  getDetectionSignalKernels() %>%
  contrastDetectionSignalKernels() %>%
  plotDetectionSignalKernels('exp2',c(0,440),c(-0.4,0.7), 'luminance (RGB units)')
```

#### Detection

Participants’ detection decisions were sensitive to fluctuations in the luminance of the target stimulus, such that ‘yes’ responses were associated with a brighter target stimulus ($E_{detection-s}$; `r apa_print(e2$RC$signal_decision_pos300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp2-RC)D, blue curve). Similar to Exp. 1, and in contrast to the behaviour of Bayes-rational simulated agents, the luminance of the non-target stimulus had a negative effect on the probability of responding ‘yes’ ($E_{detection-n}$; `r apa_print(e2$RC$signal_decision_neg300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp2-RC)D, red curve).

Confidence in detection ‘yes’ responses was similarly sensitive to fluctuations in the luminance of the target stimulus ($E_{conf-yes-s}$; `r apa_print(e2$RC$signal_confidenceYes_pos300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp2-RC)F, blue curve). Again, brighter non-target stimuli made participants less, rather than more, confident in the presence of a signal ($E_{conf-yes-n}$; `r apa_print(e2$RC$signal_confidenceYes_neg300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp2-RC)F, red curve). As in Exp. 1, here too sum evidence (overall luminance) had no significant effect on confidence in detection ‘yes’ responses ($E_{conf-yes-sum}$; `r apa_print(e2$RC$signal_confidenceYes_sum300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp2-RC)G, black curve). However, this surprising result was not replicated in Experiments 3 and 4.

Finally, unlike in Exp. 1, confidence in detection ‘no’ responses was sensitive to random fluctuations in the luminance of the target, such that participants were more confident in the absence of a signal when the target stimulus was darker ($E_{conf-no-s}$ `r apa_print(e2$RC$signal_confidenceNo_pos300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp2-RC)H). The overall luminance of the display also had a negative effect on confidence in detection ‘no’ responses ($E_{conf-no-sum}$; `r apa_print(e2$RC$signal_confidenceNo_sum300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp2-RC)I). The luminance of the non-target stimulus ($E_{conf-no-n}$; `r apa_print(e2$RC$signal_confidenceNo_neg300$diff%>%t.test())$statistic`) and the difference in luminance between the two stimuli ($E_{conf-no-relative}$; `r apa_print(e2$RC$signal_confidenceNo_rel300$diff%>%t.test())$statistic`) had no significant effects on confidence in detection ‘no’ responses.

# 4. Experiment 3

In Exp. 3 we aimed to replicate our findings using a direct experimental manipulation in addition to employing reverse-correlation analysis. Our pre-registered objectives (see our pre-registration document: [https://osf.io/hm3fn/](https://osf.io/hm3fn/)) were 1) to replicate a positive evidence bias in discrimination confidence, 2) to replicate a positive evidence bias in detection decisions, 3) to replicate the absence of a positive evidence bias in detection confidence.

## 4.1 Methods

### 4.1.1 Participants

The research complied with all relevant ethical regulations, and was approved by the Research Ethics Committee of University College London (study ID number 1260/003). 173 participants were recruited via Prolific (prolific.co), and gave their informed consent prior to their participation. They were selected based on their acceptance rate (>95%) and for being native English speakers. Following our pre-registration, we aimed to collect data until we had reached 100 included participants based on our pre-specified inclusion criteria (see [https://osf.io/hm3fn/](https://osf.io/hm3fn/)). Our final data set includes observations from 100 included participants. The entire experiment took around 20 minutes to complete. Participants were paid £2.50 for their participation, equivalent to an hourly wage of £7.50.

### 4.1.2 Experimental paradigm

Experiment 3 was identical to Experiment 2 with two changes. First, on half of the trials (high-luminance trials) the luminance of both sets of bars was increased by 2/255 for the entire duration of the display, thereby increasing sum evidence without affecting relative evidence. Second, in order to increase our statistical power for detecting response-specific effects in detection, participants performed four detection blocks and two discrimination blocks. Each block comprised 56 trials. The order of blocks was [detection, discrimination, detection, discrimination, detection, detection] for all participants.

## 4.2 Results

```{r RC-exp3-load-and-format-data, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE}

e3=list()

e3$df <- read_csv('../experiments/Experiment3/data/Flicker2.csv', lazy=FALSE) %>%
  filter(include==TRUE) %>%
  group_by(s,task,trial_index) %>%
  mutate(side=c(rep(0,48),rep(1,48)),
         timepoint=rep(1:12,8), 
         eccentricity = rep(c(rep(1,12),rep(2,12),rep(3,12),rep(4,12)),2)) %>%
  rename(subj_id = s,
         trial_id = trial_index) %>%
  mutate(detection = ifelse(task=='detection',1,0),
         bright_side=ifelse(bright_side=='right',1,0),
         signal=signal_presence,
         detection=as.factor(detection),
         response=as.factor(response),
         correct=as.factor(correct),
         subj_id=as.factor(subj_id)) %>%
  ungroup() %>%
  dplyr::select(subj_id,detection,trial_id,
                RT,signal,correct,bright_side,
                response,conf_RT,confidence,
                luminance,side,timepoint,
                eccentricity,trial, brightness_boost)

e3$trial_df <- e3$df %>%
  group_by(subj_id, trial_id) %>%
  summarise(
    detection = detection[side==1 & timepoint==1 & eccentricity == 1],
    bright_side = bright_side[side==1 & timepoint==1 & eccentricity == 1],
    signal = signal[side==1 & timepoint==1 & eccentricity == 1],
    response = response[side==1 & timepoint==1 & eccentricity == 1],
    RT = RT[side==1 & timepoint==1 & eccentricity == 1]-480, # stimulus duration
    confidence = confidence[side==1 & timepoint==1 & eccentricity == 1],
    correct = correct[side==1 & timepoint==1 & eccentricity == 1],
    trial_number = trial[side==1 & timepoint==1 & eccentricity == 1],
    brightness_boost = brightness_boost[side==1 & timepoint==1 & eccentricity == 1],
    conf_bi = ifelse(
      response==1, 
      as.numeric(confidence),
      -1*as.numeric(confidence)),
    luminance_0=mean(luminance[side==0 & timepoint<8]),
    luminance_1=mean(luminance[side==1 & timepoint<8]))%>%
  group_by(subj_id) %>%
  mutate(
    conf_discrete = ntile(confidence,20) %>%
      factor(levels=1:21),
    logRT=log(RT),
    evidence_0=(luminance_0-128)/5,
    evidence_1=(luminance_1-128)/5,
    stimulus=bright_side);

e3$detection_df <- e3$trial_df %>%
  filter(detection==1) %>%
  mutate(stimulus=signal);

e3$discrimination_df <- e3$trial_df %>%
  filter(detection==0) %>%
  mutate(stimulus = bright_side);
```

```{r RC-exp3-general-stats, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE}

e3 <- e3 %>% generalStats2Tasks() %>%
  testAUC() %>%
  testzROC2tasks();

e3$zROC <- rbind(e3$detection_zROC%>%mutate(detection=1),
                 e3$discrimination_zROC%>%mutate(detection=0));

e3$zROC_contrast_rsquare <- e3$zROC%>%group_by(subj_id)%>%summarize(fit_diff=r.squared[detection==1]-r.squared[detection==0])

```

### 4.2.1 Decision accuracy

Overall proportion correct was `r e3$task_stats%>%filter(detection==0)%>%pull(acc)%>%mean()%>%printnum()` in the discrimination and `r e3$task_stats%>%filter(detection==1)%>%pull(acc)%>%mean()%>%printnum()` in the detection task. Performance in discrimination was significantly higher than in detection (`r t.test(e3$task_stats%>%filter(detection==0)%>%pull(acc), e3$task_stats%>%filter(detection==1)%>%pull(acc), paired=TRUE)%>%apa_print()%>%'$'(full_result)`), as expected. 

### 4.2.2 Overall properties of decision and confidence distributions

Similar to Exp. 1 and 2, participants were more likely to respond 'yes' than 'no' in the detection task (mean proportion of 'yes' responses: `r mean(e3$detection_stats$general$bias)%>%printnum()`). We did not observe a consistent response bias in the discrimination task (mean proportion of 'right' responses: `r mean(e3$discrimination_stats$general$bias)%>%printnum()`). The two detection responses showed the typical asymmetries, with 'yes' responses being faster (median difference of `r e3$detection_stats$contrast_responses%>%pull(RT)%>%median()%>%abs()%>%round()` ms) and accompanied by higher levels of confidence (mean difference of `r e3$detection_stats$contrast_responses$confidence%>%mean()%>%printnum()` on a 0-1 scale). As in Exp. 1, metacognitive sensitivity was higher for ‘yes’ than for ‘no’ responses  (mean difference of `r e3$AUC%>%filter(detection==1)%>%pull(metacognitive_asymmetry)%>%mean()%>%printnum()` in AUC units). No asymmetries were observed between the two discrimination responses. For a detailed statistical analysis see Appendix.


### 4.2.3 Reverse correlation

```{r RC-exp3-discrimination-RC, cache= TRUE, echo=FALSE, message=FALSE, results='hide', warning=FALSE, fig.show='hide'}

e3$demeaned_df <- e3$df %>%
  mutate(
    evidence = ifelse(side==bright_side & signal==1,
                      (luminance-133 - brightness_boost)/5,
                      (luminance-128 - brightness_boost)/5))

e3$discRCdf <- e3$demeaned_df %>%
  filter(detection==0) %>%
  mutate(obj_side=factor(ifelse(side==bright_side, 'true','opposite'),
                         levels=c('true','opposite')),
         side=ifelse(response==side,'chosen','unchosen')) %>%
  group_by(subj_id) %>%
  mutate(median_confidence=median(confidence)) %>%
  ungroup() %>%
  mutate(
    binaryconf = ifelse(confidence>=median_confidence, 1, 0),
    time=(timepoint-1)*40) 

e3 <- e3 %>% 
  getDiscriminationKernels() %>%
  contrastDiscriminationKernels() %>%
  plotDiscriminationKernels('exp3',c(0,440),c(-0.4,0.8), 'luminance (RGB units)')
```

We first focused on reverse correlation analyses, pooling data from both high-luminance and standard trials (after mean-centering luminance in each), in order to replicate the findings of Exps. 1 and 2. We note the results are qualitatively similar when including standard trials only, with the exception of confidence in detection ‘no’ responses (see Appendix).

#### Discrimination decisions

Discrimination decisions were sensitive to relative evidence during the first 300 milliseconds of the trial ($E_{relative}$; `r apa_print(e3$RC$accuracy_rel300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp3-RC)A) with no effect of sum evidence ($E_{sum}$;`r apa_print(e3$RC$accuracy_sum300$diff%>%t.test())$statistic`).

(ref:RC-exp3-RC) Reverse correlation, Exp. 3. Same conventions as in Fig. \@ref(fig:RC-exp1-RC).

```{r RC-exp3-RC, echo=FALSE, message=FALSE, fig.cap="(ref:RC-exp3-RC)"}

knitr::include_graphics("figures/RC/exp3/summary.png")
```

#### Discrimination confidence

Decision confidence was sensitive to positive ($E_{conf-s}$; `r apa_print(e3$RC$confidence_pos300$diff%>%t.test())$statistic`) and negative ($E_{conf-n}$; `r apa_print(e3$RC$confidence_neg300$diff%>%t.test())$statistic`) evidence within the first 300 milliseconds of the stimulus (see Fig. \@ref(fig:RC-exp3-RC)B). Reverse correlation revealed no effect of random fluctuations in sum evidence on decision confidence ($E_{conf-sum}$; `r apa_print(e3$RC$confidence_sum300$diff%>%t.test())$statistic`), but an effect of sum evidence was found when directly contrasting high- and low-luminance trials, as we show in the “Evidence-weighting” Section below.

#### Detection

```{r RC-exp3-signal-RC, cache= TRUE, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}

e3$signalRCdf <- e3$demeaned_df %>%
  filter(detection==1 & signal==1) %>%
  mutate(side=factor(ifelse(side==bright_side, 'true','opposite'),
                         levels=c('true','opposite'))) %>%
  group_by(subj_id,response) %>%
  mutate(median_confidence=median(confidence)) %>%
  ungroup() %>%
  mutate(
    binaryconf = ifelse(confidence>=median_confidence, 1, 0),
    time=(timepoint-1)*40) 

e3 <- e3 %>% 
  getDetectionSignalKernels() %>%
  contrastDetectionSignalKernels() %>%
  plotDetectionSignalKernels('exp3',c(0,440),c(-0.4,0.8), 'luminance (RGB units)')
```

Participants’ detection decisions were sensitive to fluctuations in the luminance of the target stimulus, such that ‘yes’ responses were associated with brighter target stimuli ($E_{detection-s}$; `r apa_print(e3$RC$signal_decision_pos300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp3-RC)D, blue curve). Replicating the surprising results of Exp. 1 and 2, the luminance of the non-target stimulus had a negative effect on the probability of responding ‘yes’ in the detection task ($E_{detection-n}$; `r apa_print(e3$RC$signal_decision_neg300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp3-RC)D, red curve) Together, detection decisions were sensitive to relative evidence ($E_{detection-relative}$, or the difference in luminance between the target and non-target stimuli; `r apa_print(e3$RC$signal_decision_rel300$diff%>%t.test())$statistic`), and to sum evidence ($E_{detection-relative}$, or the overall luminance of the display; `r apa_print(e3$RC$signal_decision_sum300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp3-RC)E, orange and black curves, respectively).

Confidence in detection ‘yes’ responses was similarly positively correlated with the luminance of the target stimulus ($E_{conf-yes-s}$; `r apa_print(e3$RC$signal_confidenceYes_pos300$diff%>%t.test())$statistic`), and negatively correlated with the luminance of the non-target stimulus ($E_{conf-yes-s}$; `r apa_print(e3$RC$signal_confidenceYes_neg300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp3-RC)F). This is again in contrast to what is expected from a Bayes-rational agent: the probability of being correct is positively correlated with evidence intensity in both signal and non-signal channels. Recall that a surprising finding in Exp. 1 and 2 was that sum evidence (motion energy or luminance) had no effect on participants’ confidence in their judgments of stimulus presence. In contrast, in Exp. 3 sum luminance had a significant positive effect on decision confidence when reporting target presence ($E_{conf-yes-sum}$; `r apa_print(e3$RC$signal_confidenceYes_sum300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp3-RC)G, black curve).

Finally, and in line with what we observed in Exp. 2, confidence in detection ‘no’ responses was sensitive to random fluctuations in the luminance of the target, such that participants were more confident in the absence of a signal when the target stimulus was darker ($E_{conf-no-s}$; `r apa_print(e3$RC$signal_confidenceNo_pos300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp3-RC)H). Relative evidence also had a marginally significant negative effect on confidence in decisions about absence ($E_{conf-no-relative}$;  `r apa_print(e3$RC$signal_confidenceNo_rel300$diff%>%t.test())$statistic`). The luminance of the non-target stimulus and the overall luminance had no significant effects on confidence in detection ‘no’ responses (p's>0.3).

### 4.2.4 Evidence-weighting

In Experiments 1 and 2, confidence in signal presence was invariant to sum evidence (overall motion energy in Exp.1, sum luminance in Exp. 2). This was surprising for two reasons. First, in both cases sum evidence did have a significant effect on detection decisions. Second, incorporating information about sum evidence into confidence in the presence of a stimulus is rational: a target stimulus is more likely to be present (in either location) when both target *and* non-target stimuli are brighter compared to when both are dark. As we document above, however, the counterintuitive findings of Exps. 1 and 2 only partly replicated in Exp. 3: subjects still negatively weighted the luminance of the non-target stimulus (despite this being irrational), but this negative effect was weaker than the positive effect of the luminance of the target stimulus, resulting in an overall positive effect of sum evidence on detection confidence.

To shed further light on this issue, in Exp. 3 half of the trials were manipulated to include slightly brighter stimuli, thereby increasing statistical power for tests of the effects of sum luminance on discrimination and detection decisions and confidence.


```{r RC-exp3-ew, echo=FALSE, cache= TRUE, warning=FALSE, message=FALSE}

e3$discrimination_conf_PEB <- e3$trial_df %>%
  filter(detection==0) %>%
  group_by(subj_id, brightness_boost) %>%
  summarise(confidence=mean(confidence)) %>%
  group_by(subj_id)%>%
  summarise(PEB=confidence[brightness_boost==2]-confidence[brightness_boost==0]);

e3$H1$t.test <- t.test(e3$discrimination_conf_PEB$PEB);
e3$H1$d <- cohensD(e3$discrimination_conf_PEB$PEB);

e3$detection_dec_PEB <- e3$trial_df %>%
  filter(detection==1) %>%
  group_by(subj_id, brightness_boost) %>%
  summarise(P=mean(as.numeric(as.character(response))))%>%
  group_by(subj_id)%>%
  summarise(PEB=P[brightness_boost==2]-P[brightness_boost==0]);

e3$H2 <- t.test(e3$detection_dec_PEB$PEB);

e3$PEB_interaction <- e3$trial_df %>%
  filter(correct==1 & signal==1) %>%
  group_by(subj_id, detection, brightness_boost) %>%
  summarise(confidence=mean(confidence)) %>%
  group_by(subj_id,detection) %>%
  summarise(PEB=confidence[brightness_boost==2]-confidence[brightness_boost==0]) %>%
  group_by(subj_id) %>%
  summarise(PEB_diff = PEB[detection==0]-PEB[detection==1]); 

e3$H3 <- t.test(e3$PEB_interaction$PEB_diff);

e3$yes_conf_PEB <- e3$trial_df %>%
  filter(detection==1 & response==1) %>%
  group_by(subj_id, brightness_boost) %>%
  summarise(confidence=mean(confidence)) %>%
  group_by(subj_id)%>%
  summarise(PEB=confidence[brightness_boost==2]-confidence[brightness_boost==0]);


e3$H4$BF <- ttestBF(e3$yes_conf_PEB$PEB,rscale=e3$H1$d);
e3$H4$t.test <- t.test(e3$yes_conf_PEB$PEB)


e3$no_conf_PEB <- e3$trial_df %>%
  filter(detection==1 & response==0) %>%
  group_by(subj_id, brightness_boost) %>%
  summarise(confidence=mean(confidence)) %>%
  group_by(subj_id)%>%
  summarise(PEB=confidence[brightness_boost==2]-confidence[brightness_boost==0]);

e3$H5$BF <- ttestBF(e3$no_conf_PEB$PEB,rscale=e3$H1$d);
e3$H5$t.test <- t.test(e3$no_conf_PEB$PEB)
```


First, we established that participants were more likely to respond 'yes' on higher- compared to lower-luminance trials (`r apa_print(e3$H2)$full_result`), consistent with overall luminance providing a valid cue for signal presence. 

We next turned to the effects of our luminance manipulation on confidence. For discrimination judgments, participants were also more confident in higher- compared to lower- luminance trials (`r apa_print(e3$H1$t.test)$full_result`; see Fig. \@ref(fig:RC-conf-boost)A), replicating a positive evidence bias in discrimination confidence. For detection judgments, in line with the reverse correlation analysis of Exp. 3 (and in contrast to the findings of Experiments 1 and 2), participants were more confident in their 'yes' responses when overall luminance was higher (`r apa_print(e3$H4$t.test)$full_result`). Our pre-registered Bayesian analysis provided strong evidence for the alternative hypothesis that detection confidence is affected by this manipulation (`r apa_print(e3$H4$BF)$statistic`). Furthermore, this increase in confidence in presence as a function of the brightness manipulation was not significantly different from that observed for discrimination confidence (`r apa_print(e3$H3)$full_result`). Finally, and in line with Exp. 2, overall luminance had a significant negative effect on confidence in 'no' responses (`r apa_print(e3$H5$t.test)$full_result`), indicating that participants were more confident in the absence of a target when overall luminance was lower. 

(ref:conf-boost) Difference in confidence between standard and higher evidence (luminance and hue) trials for the three response categories (detection 'yes' and 'no' responses, and discrimination responses) in Exp. 3 and 4. Box edges and central lines represent the 25, 50 and 75 quantiles. Whiskers cover data points within four inter-quartile ranges around the median. Stars represent significance in a two-sided t-test: \*\*: p<0.01, *\*\*: p<0.001

```{r RC-conf-boost, echo=FALSE, cache= TRUE, message=FALSE, warning=FALSE, fig.cap="(ref:conf-boost)", fig.scap="A: Difference in confidence between standard and high-luminance trials in Exp. 3. B: Difference in confidence between standard and high-evidence trials in Exp. 4."}


conf_boost_df <- e3$trial_df %>%
  mutate(response_category = factor(ifelse(detection==0, 'discrimination', 
                                           ifelse(response==1, 'yes','no')), 
                                    levels=c('yes','no','discrimination'))) %>%
  group_by(subj_id,brightness_boost,response_category)%>%
  summarise(confidence=mean(confidence),
            RT=mean(RT),
            correct=mean(as.numeric(as.character(correct))));

e3$accuracy_boost <- conf_boost_df %>%
  filter(response_category=='discrimination') %>%
  group_by(subj_id) %>%
  summarise(accuracy_effect=correct[brightness_boost==2]-correct[brightness_boost==0]) %>%
  merge(e3$discrimination_conf_PEB)

p<-ggplot(conf_boost_df,
             aes(x=brightness_boost,y=confidence,group=brightness_boost,fill=response_category,color=response_category)) +
  ylim(0,1)+
  geom_boxplot(size=1,fill='white',outlier.alpha=0)+
  geom_jitter(alpha=0.3,size=2, width=0.3) +
  scale_color_manual(values=c(detection_colors,discrimination_colors[1]))+
  scale_fill_manual(values=c(detection_colors,discrimination_colors[1]))+
  scale_x_continuous(breaks=c(0,2))+
  theme_classic()+
  theme(legend.position='none',
        plot.margin=unit(c(0, 25, 0, 25),'pt'))+
  facet_grid(~response_category)+
  labs(x='brightness');

# ggsave('figures/confidence_boost_exp3.png',p,width=6,height=3)
knitr::include_graphics('figures/exp34conf_boost.png')

```

# 5. Experiment 4

A limitation of Exp. 2 and 3 is that apparent asymmetries in the weighting of positive and negative evidence may result from a nonlinear mapping between luminance in RGB space and screen brightness^[We thank an anonymous reviewer for bringing this issue to our attention.]. For example, a dark bar that is -2 RGB units from the mean does not necessarily cancel out a bright bar that is +2 RGB units from the mean (unless working with a gamma-corrected monitor), making positive evidence objectively more salient than negative evidence.

To address this concern, we include an additional Experiment where evidence is sampled from a perceptually uniform space. Specifically, Exp. 4 was similar to Exp. 3 with the exception that flickering stimuli varied in their hue rather than luminance, and where hue values were sampled from a Gaussian distribution in the CIE L\*a\*b\* colour space. Moreover, the roles of ‘target’ and ‘non-target’ hues were counterbalanced between participants, such that any built-in asymmetries in the perception of positive and negative evidence should cancel out at the group level.

## 5.1 Methods

### 5.1.1 Participants

The research complied with all relevant ethical regulations, and was approved by the Research Ethics Committee of University College London (study ID number 1260/003). 117 participants were recruited via Prolific (prolific.co), and gave their informed consent prior to their participation. They were selected based on their acceptance rate (>95%) and for being native English speakers. Following our pre-registration, we aimed to collect data until we had reached 100 included participants based on our pre-specified inclusion criteria (see [https://osf.io/9zbpc](https://osf.io/9zbpc)). Our final data set includes observations from 100 included participants. The entire experiment took around 20 minutes to complete. Participants were paid £2.50 for their participation, equivalent to an hourly wage of £7.50.

### 5.1.2 Experimental paradigm

Experiment 4 was identical to Experiment 3 with two changes. First, flickering bars varied in hue, randomly sampled from a Gaussian distribution in the CIE L\*a\*b\* colour space, centred at $L=54$, $a=21.5$ and $b=11.5$, with a radius of 49 [@schurgin2020psychophysical]. For half of the participants, non-target hues were sampled around an orientation of 1.85 radians with a standard deviation of 0.35, and target hues were sampled around an orientation of 2.1 with a standard deviation of 0.35. For the first group, target patches were little more orange than non-target patches, and for the second group target patches were little more green than non-target patches. To make sure non-target patches were perceived as the absence of signal relative to the background, the RSVP display was overlaid on top of a rectangle with the mean color of a non-target patch. Second, in order to avoid interference with the colour-judgment task, the confidence circle was presented in gray. Third, subjects were allowed to repeat the multiple choice questions up to three times. Finally, in addition to trial-wise feedback, block-wise feedback about overall performance and mean confidence in correct and incorrect responses was displayed at the end of each block.

```{r RC-exp4-load-and-format-data, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE}

e4=list()

#this is a hack: the hue data arrives as a python-like nested list (side, then position, then time), 
# so to use unnest_longer I first need to change it to a R-list. 
explode = function(hue_string) {
  return(gsub("\\[|\\]", "", hue_string)%>%strsplit(','))
}

flicker_mu <- c(1.85,2.1);
flicker_delta <- c(0.25,-0.25)


e4$raw_df <- read_csv('../experiments/Experiment4/data/jatos_results_batch1.csv', lazy=FALSE); 

e4$include_detection <- e4$raw_df %>%
  filter(trial_type=='flicker' & test_part=='test' & detection==1) %>%
  mutate(s=as.numeric(factor(participant_number))) %>%
  group_by(s)%>%
  summarise(acc=mean(correct),
            followed_detection=followed_detection[1],
            followed_discrimination=followed_discrimination[1]) %>%
  filter(acc>=0.55 & followed_detection & followed_discrimination) %>%
  pull(s)

e4$include_discrimination <- e4$raw_df %>%
  filter(trial_type=='flicker' & test_part=='test' & detection==0) %>%
  mutate(s=as.numeric(factor(participant_number))) %>%
  group_by(s)%>%
  summarise(acc=mean(correct),
            followed_detection=followed_detection[1],
            followed_discrimination=followed_discrimination[1]) %>%
  filter(acc>=0.55 & followed_detection & followed_discrimination) %>%
  pull(s)

e4$processed_df <- e4$raw_df %>%
  filter(trial_type=='flicker' & test_part=='test') %>%
  mutate(s=as.numeric(factor(participant_number)),
         task=ifelse(detection==1,'detection','discrimination'),
         RT=RT-480, #stimulus duration
         include=ifelse(detection==1, s %in% e4$include_detection & RT>=200, 
                        s %in% e4$include_discrimination) & RT>=200) %>%
  group_by(s)%>%
  mutate(index=seq(n()),
         block=ceiling(index/28)) %>%
  group_by(s,detection) %>%
  mutate(trial=seq(n()))%>%
  dplyr::select(s,colormapping, task,trial,index, RT,signal_presence, response, special_side, conf_RT, 
                confidence, hue, trial_index, correct,block, boost, include) %>%
  mutate(hue=explode(hue))%>%
  unnest_longer(hue) %>%
  mutate(hue=round(as.numeric(hue)*100)/100) %>%
  group_by(s,task,trial_index) %>%
  mutate(side=c(rep(0,48),rep(1,48)),
         timepoint=rep(1:12,8),
         eccentricity = rep(c(rep(1,12),rep(2,12),rep(3,12),rep(4,12)),2)) %>%
  ungroup() %>%
  mutate(
    normal_hue = (hue-flicker_mu[colormapping])/flicker_delta[colormapping],
    normal_boost = (boost/flicker_delta[colormapping])
  ) 


e4$df <- e4$processed_df %>%
  filter(include==TRUE)%>%
  rename(subj_id = s,
         trial_id = trial_index) %>%
  mutate(detection = ifelse(task=='detection',1,0),
         special_side=ifelse(special_side=='right',1,0),
         signal=signal_presence,
         detection=as.factor(detection),
         correct=as.factor(correct),
         response=as.factor(ifelse(response %in% c('y','f'),1,0)),
         subj_id=as.factor(subj_id)) %>%
  ungroup() %>%
  dplyr::select(subj_id,detection,trial_id,
                RT,signal,correct,special_side,
                response,conf_RT,confidence,
                normal_hue,side,timepoint,
                eccentricity,trial, normal_boost)

e4$trial_df <- e4$df %>%
  group_by(subj_id, trial_id) %>%
  summarise(
    detection = detection[side==1 & timepoint==1 & eccentricity == 1],
    special_side = special_side[side==1 & timepoint==1 & eccentricity == 1],
    normal_boost = normal_boost[side==1 & timepoint==1 & eccentricity == 1],
    signal = signal[side==1 & timepoint==1 & eccentricity == 1],
    response = response[side==1 & timepoint==1 & eccentricity == 1],
    RT = RT[side==1 & timepoint==1 & eccentricity == 1], # stimulus duration
    confidence = confidence[side==1 & timepoint==1 & eccentricity == 1],
    correct = correct[side==1 & timepoint==1 & eccentricity == 1],
    trial_number = trial[side==1 & timepoint==1 & eccentricity == 1],
    conf_bi = ifelse(
      response==1, 
      as.numeric(confidence),
      -1*as.numeric(confidence)),
    hue_0=mean(normal_hue[side==0 & timepoint<8]),
    hue_1=mean(normal_hue[side==1 & timepoint < 8]))%>%
  group_by(subj_id) %>%
  mutate(
    conf_discrete = ntile(confidence,20) %>%
      factor(levels=1:21),
    logRT=log(RT),
    evidence_0=hue_0,
    evidence_1=hue_1,
    stimulus=special_side);

e4$detection_df <- e4$trial_df %>%
  filter(detection==1) %>%
  mutate(stimulus=signal);

e4$discrimination_df <- e4$trial_df %>%
  filter(detection==0) %>%
  mutate(stimulus = special_side);
```


```{r RC-exp4-general-stats, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE}
e4 <- e4 %>% 
  generalStats2Tasks() %>%
  testAUC() %>%
  testzROC2tasks();


e4$zROC <- rbind(e4$detection_zROC%>%mutate(detection=1),
                 e4$discrimination_zROC%>%mutate(detection=0));

e4$zROC_contrast_rsquare <- e4$zROC%>%group_by(subj_id)%>%summarize(fit_diff=r.squared[detection==1]-r.squared[detection==0])
```

## 5.2. Results


### 5.2.1 Decision accuracy

Overall proportion correct was `r e4$task_stats%>%filter(detection==0)%>%pull(acc)%>%mean()%>%printnum()` in the discrimination and `r e4$task_stats%>%filter(detection==1)%>%pull(acc)%>%mean()%>%printnum()` in the detection task. Performance in discrimination was significantly higher than in detection (`r t.test(e4$task_stats%>%filter(detection==0)%>%pull(acc), e4$task_stats%>%filter(detection==1)%>%pull(acc))%>%apa_print()%>%'$'(full_result)`), as expected. 

### 5.2.2 Overall properties of decision and confidence distributions

Similar to Exp. 1-3, participants were more likely to respond 'yes' than 'no' in the detection task (mean proportion of 'yes' responses: `r mean(e4$detection_stats$general$bias)%>%printnum()`). A slight response bias in discrimination was not significant (mean proportion of 'right' responses: `r mean(e4$discrimination_stats$general$bias)%>%printnum()`). The two detection responses showed the typical asymmetries, with 'yes' responses being faster (median difference of `r e4$detection_stats$contrast_responses%>%pull(RT)%>%median()%>%abs()%>%round()` ms) and accompanied by higher levels of confidence (mean difference of `r e4$detection_stats$contrast_responses$confidence%>%mean()%>%printnum()` on a 0-1 scale). A mean difference of `r e4$AUC%>%filter(detection==1)%>%pull(metacognitive_asymmetry)%>%mean()%>%printnum()` in metacognitive sensitivity (AUC units) was not significant. For a detailed statistical analysis see Appendix.


### 5.2.3 Reverse correlation

```{r RC-exp4-discrimination-RC, cache= TRUE, echo=FALSE, message=FALSE, results='hide', warning=FALSE, fig.show='hide'}

e4$demeaned_df <- e4$df %>%
  mutate(
    evidence = ifelse(side==special_side & signal==1,
                      normal_hue - normal_boost - 1,
                      normal_hue - normal_boost))

e4$discRCdf <- e4$demeaned_df %>%
  filter(detection==0) %>%
  mutate(obj_side=factor(ifelse(side==special_side, 'true','opposite'),
                         levels=c('true','opposite')),
         side=ifelse(response==side,'chosen','unchosen')) %>%
  group_by(subj_id) %>%
  mutate(median_confidence=median(confidence)) %>%
  ungroup() %>%
  mutate(
    binaryconf = ifelse(confidence>=median_confidence, 1, 0),
    time=(timepoint-1)*40) 

e4 <- e4 %>% 
  getDiscriminationKernels() %>%
  contrastDiscriminationKernels() %>%
  plotDiscriminationKernels('exp4',c(0,440),c(-0.4,0.5), 'hue (radians)')
```

#### Discrimination decisions

Discrimination decisions were sensitive to relative evidence during the first 300 milliseconds of the trial ( $E_{relative}$; `r apa_print(e4$RC$accuracy_rel300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp4-RC)A). Sum evidence had a positive effect on discrimination decisions, such that subjects were more likely to correctly select the target stimulus when the overall hue of both stimuli together was closer to the target hue ($E_{sum}$; `r apa_print(e4$RC$accuracy_sum300$diff%>%t.test())$statistic`).


(ref:RC-exp4-RC) Reverse correlation, Exp. 4. Same conventions as in Fig. \@ref(fig:RC-exp1-RC).

```{r RC-exp4-RC, echo=FALSE, message=FALSE, fig.cap="(ref:RC-exp4-RC)"}

knitr::include_graphics("figures/RC/exp4/summary.png")
```

#### Discrimination confidence 

Decision confidence was sensitive to positive ($E_{conf-chosen}$; `r apa_print(e4$RC$confidence_pos300$diff%>%t.test())$statistic`) and negative ($E_{conf-unchosen}$;`r apa_print(e4$RC$confidence_neg300$diff%>%t.test())$statistic`) evidence within the first 300 milliseconds (see Fig. \@ref(fig:RC-exp4-RC)B). The effect of sum evidence on decision confidence was only marginally significant ($E_{conf-sum}$; `r apa_print(e4$RC$confidence_sum300$diff%>%t.test())$statistic`). An effect of sum evidence was found when directly contrasting high- and low- evidence trials, as we show in the “Evidence-weighting” Section below.

```{r RC-exp4-signal-RC, cache= TRUE, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}

e4$signalRCdf <- e4$demeaned_df %>%
  filter(detection==1 & signal==1) %>%
  mutate(side=factor(ifelse(side==special_side, 'true','opposite'),
                         levels=c('true','opposite'))) %>%
  group_by(subj_id,response) %>%
  mutate(median_confidence=median(confidence)) %>%
  ungroup() %>%
  mutate(
    binaryconf = ifelse(confidence>=median_confidence, 1, 0),
    time=(timepoint-1)*40) 

e4 <- e4 %>% 
  getDetectionSignalKernels() %>%
  contrastDetectionSignalKernels() %>%
  plotDetectionSignalKernels('exp4',c(0,440),c(-0.4,0.5), 'hue (radians)')
```

#### Detection

Participants’ detection decisions were sensitive to fluctuations in the hue of the target stimulus ($E_{detection-s}$; `r apa_print(e4$RC$signal_decision_pos300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp4-RC)D, blue curve). Fluctuations in the hue of the non-target stimulus had the opposite effect on detection decisions, replicating again the results of Exp. 1-3 ($E_{detection-n}$; `r apa_print(e4$RC$signal_decision_neg300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp4-RC)D, red curve).

Confidence in detection ‘yes’ responses was positively sensitive to fluctuations in the hue of the target ($E_{conf-yes-s}$; `r apa_print(e4$RC$signal_confidenceYes_pos300$diff%>%t.test())$statistic`) and negatively sensitive to fluctuations in the hue of the non-target stimulus ($E_{conf-yes-n}$; `r apa_print(e4$RC$signal_confidenceYes_neg300$diff%>%t.test())$statistic`; see Fig. 13F). Similar to Exp. 3, here too sum evidence (deviation from the background hue toward the target hue) had a significant positive effect on decision confidence when reporting target presence ($E_{conf-yes-sum}$; `r apa_print(e4$RC$signal_confidenceYes_sum300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp4-RC)G).

Finally, confidence in detection ‘no’ responses was sensitive to random fluctuations in the hue of the target, such that participants were more confident in the absence of a signal when the target stimulus was closer in hue to the background ($E_{conf-no-s}$; `r apa_print(e4$RC$signal_confidenceNo_pos300$diff%>%t.test())$statistic`; see Fig. \@ref(fig:RC-exp4-RC)H). Sum evidence (the overall hue of the display) had a marginal negative effect on confidence in absence (`r apa_print(e4$RC$signal_confidenceNo_sum300$diff%>%t.test())$statistic`). Relative evidence and negative evidence had no significant effects on confidence in detection ‘no’ responses (p's > 0.3).

### 5.2.4 Evidence-weighting

```{r RC-exp4-ew, echo=FALSE, warning=FALSE, message=FALSE, cache= TRUE,}

e4$discrimination_conf_PEB <- e4$trial_df %>%
  filter(detection==0) %>%
  group_by(subj_id, normal_boost) %>%
  summarise(confidence=mean(confidence)) %>%
  group_by(subj_id)%>%
  summarise(PEB=confidence[normal_boost==0.4]-confidence[normal_boost==0]);

e4$H1$t.test <- t.test(e4$discrimination_conf_PEB$PEB);
e4$H1$d <- cohensD(e4$discrimination_conf_PEB$PEB);

e4$detection_dec_PEB <- e4$trial_df %>%
  filter(detection==1) %>%
  group_by(subj_id, normal_boost) %>%
  summarise(P=mean(as.numeric(as.character(response))))%>%
  group_by(subj_id)%>%
  summarise(PEB=P[normal_boost==0.4]-P[normal_boost==0]);

e4$H2 <- t.test(e4$detection_dec_PEB$PEB);

e4$PEB_interaction <- e4$trial_df %>%
  filter(correct==1 & signal==1) %>%
  group_by(subj_id, detection, normal_boost) %>%
  summarise(confidence=mean(confidence)) %>%
  group_by(subj_id,detection) %>%
  summarise(PEB=confidence[normal_boost==0.4]-confidence[normal_boost==0]) %>%
  group_by(subj_id) %>%
  summarise(PEB_diff = PEB[detection==0]-PEB[detection==1]); 

e4$H3 <- t.test(e4$PEB_interaction$PEB_diff);

e4$yes_conf_PEB <- e4$trial_df %>%
  filter(detection==1 & response==1) %>%
  group_by(subj_id, normal_boost) %>%
  summarise(confidence=mean(confidence)) %>%
  group_by(subj_id)%>%
  summarise(PEB=confidence[normal_boost==0.4]-confidence[normal_boost==0]);


e4$H4$BF <- ttestBF(e4$yes_conf_PEB$PEB,rscale=e4$H1$d);
e4$H4$t.test <- t.test(e4$yes_conf_PEB$PEB)


e4$no_conf_PEB <- e4$trial_df %>%
  filter(detection==1 & response==0) %>%
  group_by(subj_id, normal_boost) %>%
  summarise(confidence=mean(confidence)) %>%
  group_by(subj_id)%>%
  summarise(PEB=confidence[normal_boost==0.4]-confidence[normal_boost==0]);

e4$H5$BF <- ttestBF(e4$no_conf_PEB$PEB,rscale=e4$H1$d);
e4$H5$t.test <- t.test(e4$no_conf_PEB$PEB)

```

As in Exp. 3, on half of the trials (‘high-evidence’ trials), the hue of both patches was slightly shifted in the direction of the target stimulus (that is, made greener if the target stimulus was greener than the non-target stimulus, or more orange otherwise). This allowed us to directly measure how sum evidence affects both detection decisions, and detection and discrimination confidence ratings. Overall, we obtained a similar pattern to Exp. 3: on high-evidence trials, participants were more likely to respond ‘yes’ in the detection task `r apa_print(e4$H2)$full_result`; see Fig. \@ref(fig:RC-conf-boost)B), and became more confident in their discrimination judgments (`r apa_print(e4$H1$t.test)$full_result`), more confident in their detection ‘yes’ responses (`r apa_print(e4$H4$t.test)$full_result`), and less confident in detection ‘no’ responses `r apa_print(e4$H5$t.test)$full_result`). Our pre-registered Bayesian analysis provided strong evidence for the alternative hypothesis that detection confidence is affected by sum evidence (`r apa_print(e4$H4$BF)$statistic`). One difference in comparison to Exp. 3 was that in Exp. 4 the increase in ‘yes’ response confidence as a function of the hue manipulation was significantly stronger than that observed for discrimination confidence (`r apa_print(e4$H3)$full_result`).


```{r RC-exp4-conf-boost, echo=FALSE, cache= TRUE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}


conf_boost_df <- e4$trial_df %>%
  mutate(response_category = factor(ifelse(detection==0, 'discrimination', 
                                           ifelse(response==1, 'yes','no')), 
                                    levels=c('yes','no','discrimination'))) %>%
  group_by(subj_id,normal_boost,response_category)%>%
  summarise(confidence=mean(confidence),
            RT=mean(RT),
            correct=mean(as.numeric(as.character(correct))));

acc_boost_df <- e4$trial_df %>%
  group_by(detection, subj_id,normal_boost) %>%
  summarise(correct=mean(as.numeric(as.character(correct)))) %>%
  spread(normal_boost,correct,sep='_') %>%
  mutate(diff=normal_boost_0.4-normal_boost_0)

e4$accuracy_boost <- conf_boost_df %>%
  filter(response_category=='discrimination') %>%
  group_by(subj_id) %>%
  summarise(accuracy_effect=correct[normal_boost==0.4]-correct[normal_boost==0]) %>%
  merge(e4$discrimination_conf_PEB)

p<-ggplot(conf_boost_df,
             aes(x=normal_boost,y=confidence,group=normal_boost,fill=response_category,color=response_category)) +
  ylim(0,1)+
  geom_boxplot(size=1,fill='white',outlier.alpha=0)+
  geom_jitter(alpha=0.3,size=2, width=0.05) +
  scale_color_manual(values=c(detection_colors,discrimination_colors[1]))+
  scale_fill_manual(values=c(detection_colors,discrimination_colors[1]))+
  scale_x_continuous(breaks=c(0,0.4))+
  theme_classic()+
  theme(legend.position='none',
        plot.margin=unit(c(0, 25, 0, 25),'pt'))+
  facet_grid(~response_category)+
  labs(x='hue');

ggsave('figures/confidence_boost_exp4.png',p,width=6,height=3)
# knitr::include_graphics('figures/confidence_boost_enhanced_color_headings.png')

```

 
# Discussion

In four experiments, we compared the drivers of decisions and confidence ratings in perceptual discrimination and detection, in conditions either matched for difficulty (Exp. 1) or signal strength (Exp. 2-4). In order to measure the contribution of perceptual evidence to confidence in detection and discrimination judgments, we followed @zylberberg2012construction in applying reverse correlation analysis to noisy stimuli in perceptual decision making tasks. We fully replicated the main results of Zylberberg and colleagues: decisions and confidence were affected by perceptual evidence in the first 300 milliseconds of the trial, peaking at around 200 milliseconds. We also successfully replicated a positive evidence bias (PEB) for discrimination confidence: confidence in the discrimination task was more affected by supporting than by conflicting evidence – a pattern which may be indicative of a detection-like decision rule operating for discrimination confidence.  This effect was qualitatively accounted for by two Bayes-rational decision making models: a firing rate model, in which perceptual noise was stimulus dependent (inspired by @miyoshi2020decision), and a goal-directed attention model (inspired by @sepulveda2020visual).

These same two models also made corresponding predictions for the detection task: when attempting to detect signal presence in either channel, decisions and confidence ratings should positively weigh evidence for both alternatives (e.g., motion energy to the right and to the left). Paradoxically, however, in the detection task subjects adopted a discrimination-like disposition, negatively weighing evidence in the non-signal channel. In other words, subjects were *less* likely to say a target was present (in either channel) when the weaker channel had *more* evidence. In Exp. 1 and 2, this negative weighting of evidence in the non-signal channel was strong enough to bring the effect of sum evidence on detection confidence down to zero (as the surprising negative effect of non-signal evidence canceled out the expected positive influence of signal evidence on detection probability). This negative weighting of evidence in the non-signal channel remained significant in Experiments 3 and 4, although was now somewhat weaker than the positive weighting of the evidence in the signal channel, leading to an overall sum evidence effect on detection probability. Overall, then, subjects incorporated detection-relevant evidence into their confidence in discrimination judgments, and discrimination-relevant evidence into their detection judgments and confidence ratings.

What drives these discrimination-like evidence weighting profiles in detection? In Experiments 3 and 4, one explanation is that our evidence-boost manipulation may have rendered it rational for subjects to focus on the difference in evidence between the two sensory channels. If on a random subset of trials both stimuli are made brighter, focusing on overall brightness is not as informative as focusing on the contrast between the brightness of the two stimuli, which remains unaffected by the evidence-boost manipulation. This account fails to explain, however, the emergence of a negative effect of evidence in the non-signal channel in Experiments 1 and 2, where the evidence-boost manipulation was not applied and where a rational agent should have positively weighted evidence from both channels.

Alternatively, changes to the global perception of overall stimulus intensity may have an internal source. For example, slow brain oscillations in the alpha band affect both detection criterion and discrimination confidence, but have minimal effects on discrimination sensitivity: a nonselective effect on perception which has been attributed to a global change in the baseline firing rate of sensory neurons [@samaha2020spontaneous]. Similar to our evidence-boost manipulation, an overall increase in baseline firing rate increases sum evidence without affecting relative evidence. If agents do not have metacognitive access to the current excitability of their perceptual system but do know that such global effects exist, focusing on relative evidence in detection may be a rational way of dealing with this ambiguity of baseline excitability. This account fails to explain, however, why subjects in Exp. 2-4 did not use the static background rather than the non-target stimulus as a reference point, given that it is presumably also susceptible to perceptual influences from global changes in the baseline firing rate of sensory neurons. 

Finally, it may be that evidence accumulation in detection and discrimination depends on shared processes and internal representations. Outside of a laboratory setting, detectability and discriminability mostly go hand in hand; the farther away from ‘nothing’ a representation is, the more distinct and differentiated from other representations it becomes. Given these meta-level expectations about the distribution of evidence in the world, the overall availability of evidence may be a valid cue for confidence in discrimination judgments [@maniscalco2016heuristic]. Conversely, asymmetries in the availability of evidence for two competing hypotheses may serve as a valid cue for the presence of signal in one of the channels. 

If discrimination confidence and detection decisions are drawing on shared evidence weighting mechanisms, one might expect that person-specific tendencies to rely more on one or other evidence channel will be correlated across both tasks. For example, subjects whose discrimination confidence was strongly affected by sum evidence (or equivalently, showed a pronounced positive evidence bias), may also be sensitive to sum evidence in their detection decisions and confidence. Surprisingly, however, we find no evidence for such an effect (see Appendix). Across subjects, the effects of positive, negative, sum and relative evidence on discrimination confidence were not reliably correlated with their corresponding effects on detection decisions, nor with their effects on confidence in signal presence. This null result should be interpreted with caution: our experiments were not powered to identify correlations between participants, with Exp. 1 adopting a small-N, many-trials design, and Experiments 2-4 a high-N, few-trials design, with the attendant limitation of noisy single-subject estimates. Thus while our current results do not directly support a shared-resources account, they are not inconsistent with it.

# Conclusion

In four experiments, we replicated previous findings of a "positive evidence bias": a detection-like evidence weighting in discrimination confidence. This pattern was accounted for by models that posit asymmetries either in the distributions of sensory noise or allocation of attention between target and non-target channels. However, these same models could not account for a surprising finding of discrimination-like evidence weighting in detection decisions and confidence. We suggest that these seemingly irrational positive and negative evidence biases may reflect, at least in part, shared representational resources being harnessed for detection decisions and discrimination confidence.


# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

# Open Practices Statement

The data and materials for all experiments, including demos of the experiments, full analysis code, model simulations, and a fully-reproducible version of the manuscript in Rmarkdown are available at github.com/matanmazor/reverseCorrelation. All four experiments were pre-registered (Exp. 1: https://osf.io/z2s93/ ; Exp. 2: https://osf.io/d3vkm/; Exp. 3: https://osf.io/hm3fn/; Exp. 4: https://osf.io/9zbpc). To ensure preregistration time-locking (in other words, that preregistration preceded data collection), we employed randomization-based preregistration. We used the SHA256 cryptographic hash function to translate our preregistered protocol folder (including the pre-registration document) to a string of 256 bits. These bits were then combined with the unique identifiers of single subjects, and the resulting string was used as seed for initializing the Mersenne Twister pseudorandom number generator prior to determining all random aspects of the experiment, including the order of trials, motion energy in Exp. 1, random luminance values in Exp 2 and 3, and hue values in Exp. 4. This way, experimental randomization was causally dependent on, and therefore could not have been determined prior to, the specific contents of our preregistration document [@mazor2019novel]. Protocol folders and their hashed sums, as well as the relevant lines of code in our Experiment code, are available [on the project's Github page](https://github.com/matanmazor/reverseCorrelation#pre-registration-time-locking-).

\newpage

# (APPENDIX) Appendix {-}

```{r child = "appendix.rmd"}
```


---
title             : "Paradoxical evidence weighting in confidence judgments for detection and discrimination"
shorttitle        : "Paradoxical evidence weighting in confidence judgments for detection and discrimination"

author: 
  - name          : "Matan Mazor"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "12 Queen Square, London WC1N 3BG"
    email         : "mtnmzor@gmail.com"
  - name          : "Lucie Charles"
    affiliation   : "4"
  - name          : "Roni Maimon-Mor"
    affiliation   : "4" 
  - name          : "Stephen M. Fleming"
    affiliation   : "1,2,3"

affiliation:
  - id            : "1"
    institution   : "Wellcome Centre for Human Neuroimaging, UCL"
  - id            : "2"
    institution   : "Max Planck UCL Centre for Computational Psychiatry and Ageing Research"
  - id            : "3"
    institution   : "Department of Experimental Psychology, UCL"
  - id            : "4"
    institution   : "Institute of Cognitive Neuroscience, UCL"

authornote: |
  The authors have no conflicting interests to declare.

abstract: | 
  When making discrimination decisions between two stimulus categories, subjective confidecne judgments are more positively affected by evidence in support of a decision than negatively affected by evidence against it. Recent theoretical proposals suggest that this “positive evidence bias” may be due to observers adopting a detection-like strategy when rating their confidence, one that has functional benefits for metacognition in real-world settings where detectability and discriminability often go hand in hand. However, it is unknown whether, or how, this evidence weighting asymmetry is also in play for detection decisions about the presence or absence of a stimulus. In three experiments (one lab-based and two online) we first successfully replicate a positive evidence bias in discrimination confidence. We then show that detection decisions and confidence ratings paradoxically suffer from an opposite “negative evidence bias” to negatively weigh evidence even when it is optimal to assign it a positive weight. We show that the two effects are uncorrelated, and discuss our findings in relation to models that account for a positive evidence bias as emerging from a confidence-specific heuristic, and alternative models where decision and confidence are generated by the same, Bayes-rational process.

  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "confidence, detection, metacognition"
wordcount         : "X"

bibliography      : ["RC.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_word


---

```{r RC_load_pkgs, echo=FALSE, message=FALSE, include=FALSE}
# List of packages required for this analysis
pkg <- c("dplyr", "ggplot2", "knitr", "bookdown")
# Check if packages are not installed and assign the
# names of the packages not installed to the variable new.pkg
new.pkg <- pkg[!(pkg %in% installed.packages())]
# If there are any packages in the list that aren't installed,
# install them
if (length(new.pkg)) {
  install.packages(new.pkg, repos = "https://cran.rstudio.com")
}
# Load packages

library("papaja")
r_refs("r-references.bib")
library('tidyverse')
library('broom')
library('cowplot')
library('MESS') # for AUCs
library('lsr') # for effect sizes
library('pwr') # for power calculations
library('brms') # for mixed effects modeling
library('BayesFactor') # for Bayesian t test
library('jsonlite') #parsing data from sort_trial
library('knitr')
library('egg')
library('zoo') # for rollapply
library('RColorBrewer')
library('reticulate') # for python

source("trialLevelAnalysisFunctions.R", local = knitr::knit_global())
source("visualizationFunctions.R", local = knitr::knit_global())
source("reverseCorrelationFunctions.R", local = knitr::knit_global())

```

# Introduction

When considering two alternative hypotheses, the probability of the chosen hypothesis to be correct is a function of the availability of evidence supporting not only the chosen hypothesis, but also the unchosen one. For example, when deciding that there are more ants in the kitchen than in the living room, confidence should not only positively weigh the number of ants found in the kitchen (*positive evidence*), but also negatively weigh the number of ants found in the living room (*negative evidence*). Specifically, a decision should be based on the difference in the number of ants between the kitchen and the living room, but not on the total number of ants found in both rooms together (we refer to these quantities as *relative evidence* and *sum evidence*, respectively).

While sum evidence is irrelevant to discrimination decisions between two symmetrical hypotheses (e.g., kitchen or living room), it is highly informative with respect to detection decisions about the presence or absence of a signal. For example, when deciding that an ant colony is nesting in the house, the total number of ants found in both rooms is highly relevant, even more so than the difference between the number of ants found in the kitchen or living room (see Fig. \@ref(fig:RC-2d-models) for a signal detection formulation).

(ref:RC-2dmodels) Discrimination and detection in a two-dimensional Signal Detection Theory model. Left: in a two-dimensional SDT model, evidence $e$ is sampled from one of two Gaussian distributions (here centered at (0,1) and (1,0)). We define relative evidence as $e_{S1}-e_{S2}$ and sum evidence as $e_{S1}+e_{S2}$. Circles represent contours of two-dimensional distributions. Center and Left: response and confidence accuracy are maximized when based on a log-likelihood ratio for the two stimulus categories. Center: in discrimination, this yields optimal decision and confidence criteria that are based on relative evidence (distance from the main diagonal), irrespective of sum evidence. Right: in detection, this yields optimal decision and confidence that are based on a non-linear interaction between relative and sum evidence. The third circle centred at (0,0) represents the two-dimensional distribution of percepts in the absence of stimuli.

```{r RC-2d-models, fig.cap="(ref:RC-2dmodels)"}

knitr::include_graphics('figures/2dmodel_enhanced.png')

```



A surprising finding is that despite the irrelevance of sum evidence to the accuracy of discrimination decisions, people are systematically more confident in perceptual decisions when sum evidence is high. For example, @zylberberg2012construction had subjects judge which of two flickering stimuli was brighter on average. Subjects were more confident in their decisions when both stimuli were bright, indicating an effect of sum evidence (here, overall luminance) on decision confidence. A positive effect of sum evidence on decision confidence is mathematically equivalent to a disproportional weighting of positive evidence over negative evidence, also known as a positive evidence bias [@koizumi2015does; @peters2017perceptual; @rollwage2020confidence; @samaha2020positive; @sepulveda2020visual; @zylberberg2012construction]. The two are equivalent because positively weighing the sum of positive and negative evidence effectively weakens the negative contribution of negative evidence to decision confidence, while strengthening the contribution of positive evidence. Notably, an effect of sum evidence on discrimination confidence may indicate a profound link between discrimination confidence and detection decisions [@rausch2018confidence].

Different models identify the origin of this evidence weighting asymmetry at different levels of the cognitive hierarchy, going from a metacognitive bias to ignore conflicting information [metacognitive level; @peters2017perceptual; @maniscalco2016heuristic], through asymmetries in the active sampling of evidence [attention allocation level; @sepulveda2020visual], down to perceptual asymmetries between the representations of signal and noise [perception level; @miyoshi2020decision; @webb2021task]. These models vary in whether they postulate separate evidence accumulation processes for decision and confidence judgments, and in whether they model confidence formation as following a suboptimal heuristic, or alternatively as being optimal with respect to available information (information which may be limited or corrupted by noise).

Here we focus on a subset of models which assume that subjects are rational decision makers equipped with veridical beliefs about the world, but that they only have limited access to noisy evidence. Our models further assume that subjects’ confidence ratings are Bayesian estimates of the probability of being correct, given the exact same evidence that was used to make the decision. The models do not postulate any metacognitive biases, heuristics, or suboptimalities. We show that two of these models reproduce a positive evidence bias (that is, a positive effect of sum evidence) in discrimination confidence. The same models also make predictions for evidence weighting in detection judgments and confidence ratings. In four experiments, reverse correlation reveals evidence weighting patterns that only partly agree with the predictions of our models. Most notably, our four models fail to account for a negative evidence bias in detection decisions and confidence. In what follows we first describe the four models and the predictions they make, before turning to empirical findings from our four experiments.

# Computational models

We model a setting in which agents are presented with a sequence of samples from two sensory channels: $E_1$ and $E_2$. The agents’ task is to decide which of the two channels was the signal channel (discrimination), or whether any of the channels had signal in it at all (detection). When a signal is present in a channel, evidence E is sampled from a normal distribution $\mathcal{N}(0.5,1)$, and when a signal is absent evidence is sampled from $\mathcal{N}(0,1)$ (see Fig. \@ref(fig:RC-models), upper panel). In all four models agents only have access to a noisy version of these samples $E'$, corrupted by sensory noise. After each time step, they update their belief about the relative likelihood of the observed samples under the two possible world states (signal in channel 1 versus 2, or signal presence versus absence), and given full knowledge of the true sample-generating process, including the properties of sensory noise. Each trial comprises 12 time steps. At the end of a trial, agents report the world state that maximizes the likelihood of the observed evidence, and rate their confidence as the objective probability that their decision was correct given likelihood estimates. The four models vary in the properties of sensory noise, and in the selection of some channels for inspection by selection mechanisms.

(ref:RC-models) Computational models. Upper panel: True world model. Stimuli span 12 timepoints, each comprising values from two sensory channels (here presented as luminance values). In discrimination blocks, values in one channel are sampled from the noise distribution (red), and values in the other channel are sampled from the signal distribution (blue). In detection blocks, on half of the trials all values are sampled from the noise distribution (red). Vanilla model: on each timepoint, participants perceive both channels, corrupted by sensory noise that is sampled from a normal distribution. They then update their beliefs accordingly. Stimulus-dependent noise model: the standard deviation of the sensory noise distribution is exponential with respect to signal intensity. Random attention model: agents only attend one channel at a time. The attended channel is chosen at random per timepoint, with a strong bias to be consistent within a trial. Goal-directed attention model: channels that are likely to include signal based on previous samples are more likely to be attended.

```{r RC-models, fig.cap="(ref:RC-models)"}

knitr::include_graphics('figures/models.png')

```

## Vanilla

In the basic, vanilla model, sensory noise is sampled from a normal distribution $\mathcal{N}(0,2)$. This model corresponds to a standard equal-variance signal detection setting, as illustrated in Fig. \@ref(fig:RC-2d-models).

## Stimulus-dependent noise

The stimulus-dependent noise model is similar to the vanilla model, with the exception that perceptual noise increases with signal intensity (formally, perceptual noise is sampled from a normal distribution $\mathcal{N}(0,2^{E})$). This assumption is biologically plausible: the representation of signal is expected to be inherently more variable due to the Weber-Fechner law [@fechner1860elements] and the coupling between firing rate mean and variability implied by a Poisson form for neuronal firing rates. Identifying the origin of the positive evidence bias at the perceptual level, this model shares family resemblance with the perceptual model of @miyoshi2020decision. However, two important differences are worth pointing out. First, here perceptual noise is conditioned not on stimulus class, but on the perceptual sample. This seems more plausible, as the perceptual system has no access to stimulus class beyond the information that is available in perceptual samples. And second, here we assume that confidence ratings are made based on all available evidence in a Bayes-rational way, rather than following a heuristic.

## Random attention

In this model, sensory noise is again sampled from $\mathcal{N}(0,2)$, irrespective of signal intensity. Unlike the vanilla model, however, here agents have access to one channel per timepoint only (they “attend” to one channel at a time). On each trial, agents are biased to attend to one preferred channel, chosen at random. Then, on each timepoint, they attend to one channel (the preferred channel with probability 0.95), and update their beliefs accordingly. We include this model because it is inherently asymmetrical: on each trial, evidence from the preferred channel contributes more to decision and confidence.

## Goal-directed attention

This model is similar to the sensory noise model, except that here attention is biased to channels that are more likely to include signal. Specifically, agents track the log likelihood ratio $LLR_r$ between signal in the left or in the right channels, and the probability of attending the right channel is set to $S(LLR_r$ where $S$ is a sigmoid function with a steep slope of 5. A highly similar drift diffusion model was shown to produce a positive evidence bias in confidence ratings [@sepulveda2020visual].

```{r RC-analyze-simulations, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}

sim = list()

sim$df <- read_csv('../simulations/equal_variance/discrimination.csv') %>%
  mutate(detection=0, eccentricity='ev', signal=1, subj_id=floor(trial_id/100)) %>%
  dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                eccentricity,evidence,decision,confidence, correct) %>%
  rbind(read_csv('../simulations/equal_variance/detection.csv') %>%
  mutate(detection=1, eccentricity='ev',subj_id=floor(trial_id/100))%>%
    dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                  eccentricity,evidence,decision,confidence, correct)) %>%
  rbind(read_csv('../simulations/unequal_variance/discrimination.csv') %>%
  mutate(detection=0, eccentricity='uv',subj_id=floor(trial_id/100),signal=1)%>%
    dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                  eccentricity,evidence,decision,confidence, correct))%>%
  rbind(read_csv('../simulations/unequal_variance/detection.csv') %>%
  mutate(detection=1, eccentricity='uv',subj_id=floor(trial_id/100))%>%
    dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                  eccentricity,evidence,decision,confidence, correct)) %>%
  rbind(read_csv('../simulations/random_attention/discrimination.csv') %>%
  mutate(detection=0, eccentricity='ra',subj_id=floor(trial_id/100),signal=1)%>%
    dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                  eccentricity,evidence,decision,confidence, correct))%>%
  rbind(read_csv('../simulations/random_attention/detection.csv') %>%
  mutate(detection=1, eccentricity='ra',subj_id=floor(trial_id/100))%>%
    dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                  eccentricity,evidence,decision,confidence, correct)) %>%
  rbind(read_csv('../simulations/goal_directed_attention/discrimination.csv') %>%
  mutate(detection=0, eccentricity='gda',subj_id=floor(trial_id/100),signal=1)%>%
    dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                  eccentricity,evidence,decision,confidence, correct))%>%
  rbind(read_csv('../simulations/goal_directed_attention/detection.csv') %>%
  mutate(detection=1, eccentricity='gda',subj_id=floor(trial_id/100))%>%
    dplyr::select(detection,subj_id,trial_id,bright_side,signal,timepoint,side,
                  eccentricity,evidence,decision,confidence, correct)) %>%
  mutate(eccentricity=factor(eccentricity, levels=c('ev','uv','ra','gda')))

sim$demeaned_df <- sim$df %>%
  mutate(evidence=ifelse(signal==1 & side==bright_side,
                evidence-0.5,
                evidence),
         time=(timepoint-1));

sim$discRCdf <- sim$demeaned_df %>%
  filter(detection==0) %>%
  mutate(obj_side=factor(ifelse(side==bright_side,
                         'true',
                         'opposite'),levels=c('true','opposite')),
         side = factor(ifelse(decision==side,
                       'chosen',
                       'unchosen'),
                       levels=c('chosen','unchosen'))) %>%
  dplyr::select(subj_id,timepoint,obj_side,side,eccentricity,confidence, trial_id,evidence, time, correct) %>%
  group_by(subj_id) %>%
  mutate(median_confidence=median(confidence)) %>%
  ungroup() %>%
  mutate(
    binaryconf = ifelse(confidence>=median_confidence, 1, 0),
  );

sim$signalRCdf <- sim$demeaned_df %>%
  filter(detection==1 & signal==1) %>%
  mutate(side=factor(ifelse(side==bright_side,
                         'true',
                         'opposite'),levels=c('true','opposite')),
         response=decision) %>%
  dplyr::select(subj_id,timepoint,side,eccentricity,confidence, trial_id,evidence, time, correct,response) %>%
  group_by(subj_id,response) %>%
  mutate(median_confidence=median(confidence)) %>%
  ungroup() %>%
  mutate(
    binaryconf = ifelse(confidence>=median_confidence, 1, 0)
  );

sim <- sim %>% 
  getDiscriminationKernels() %>%
  contrastDiscriminationKernels()

sim$discrimination_accuracy_kernel %>%
  group_by(eccentricity,subj_id,contrast) %>%
  summarise(evidence=mean(evidence)) %>%
  group_by(eccentricity,contrast) %>%
  summarise(mean_evidence=mean(evidence),
            se_evidence=se(evidence)) %>%
  ggplot(aes(x=eccentricity,y=mean_evidence,fill=contrast, 
             shape=eccentricity)) +
  geom_abline(intercept=0, slope=0,size=1) +
  geom_errorbar(aes(ymin=mean_evidence-se_evidence,
                    ymax=mean_evidence+se_evidence), 
                color='black')+
  geom_point(size=6) +
  scale_shape_manual(values=c(21,22,23,24))+
  scale_color_manual(values=evidence_colors)+
  scale_fill_manual(values=evidence_colors) +
  scale_y_continuous(limits=c(-0.2,0.5)) +
  theme_minimal()+theme(
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank()) +
  labs(y='contrast',
       x='')+
  theme(legend.position = 'none');
  
  ggsave(paste('./figures/RC/sim/discrimination_accuracy.png',sep='/'),
         width=3.5,height=3)

sim$discrimination_confidence_kernel%>%
  group_by(side,eccentricity, subj_id) %>%
  summarise(evidence=mean(diff)) %>%
  group_by(side,eccentricity)%>%
  summarise(se=se(evidence),
            evidence=mean(evidence)) %>%
  ggplot(aes(x=eccentricity,y=evidence,fill=side,shape=eccentricity)) +
    geom_hline(yintercept=0)  +
  geom_abline(intercept=0, slope=0,size=1) +
  geom_errorbar(aes(ymin=evidence-se,
                    ymax=evidence+se), 
                color='black')+
  geom_point(size=6) +
  scale_shape_manual(values=c(21,22,23,24))+
  scale_color_manual(values=discrimination_colors)+
  scale_fill_manual(values=discrimination_colors) +
  scale_y_continuous(limits=c(-0.2,0.5)) +
  theme_minimal()+theme(
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank()) +
  labs(y='evidence',
       x='')+
  theme(legend.position = 'none');

  ggsave(paste('./figures/RC/sim/discrimination_confidence.png',sep='/'),
         width=3.5,height=3)

  sim$discrimination_confidence_kernel %>% 
    group_by(eccentricity, subj_id) %>%
    summarise(relative_evidence=mean(diff[side=='chosen'])-mean(diff[side=='unchosen']),
              sum_evidence = mean(diff[side=='chosen'])+mean(diff[side=='unchosen'])) %>%
    pivot_longer(cols=ends_with('evidence'), names_to='contrast', values_to='evidence') %>%
    group_by(contrast,eccentricity) %>%
    summarise(se=se(evidence),
              evidence=mean(evidence)) %>%
    ggplot(aes(x=eccentricity,y=evidence,fill=contrast, shape=eccentricity)) +
    geom_abline(intercept=0, slope=0,size=1) +
    geom_errorbar(aes(ymin=evidence-se,
                      ymax=evidence+se), 
                  color='black')+
    geom_point(size=6) +
    scale_shape_manual(values=c(21,22,23,24))+
    scale_color_manual(values=evidence_colors)+
    scale_fill_manual(values=evidence_colors) +
    scale_y_continuous(limits=c(-0.2,0.5)) +
    theme_minimal()+theme(
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()) +
    labs(y='contrast',
         x='')+
    theme(legend.position = 'none');

  ggsave(paste('./figures/RC/sim/discrimination_confidence_sum_rel.png',sep='/'),
         width=3.5,height=3)
  
  sim <- sim %>% 
  getDetectionSignalKernels() %>%
  contrastDetectionSignalKernels()
  
  sim$signal_decision_kernel %>%
    group_by(side,eccentricity, subj_id) %>%
    summarise(evidence = mean(evidence)) %>%
    group_by(side,eccentricity) %>%
    summarise(se=se(evidence),
              evidence=mean(evidence)) %>%
  ggplot(aes(x=eccentricity,y=evidence,fill=side,shape=eccentricity))+
  geom_abline(intercept=0, slope=0,size=1) +
  geom_errorbar(aes(ymin=evidence-se,
                    ymax=evidence+se), 
                color='black')+
  geom_point(size=6) +
  scale_shape_manual(values=c(21,22,23,24))+
  scale_color_manual(values=detection_colors)+
  scale_fill_manual(values=detection_colors) +
  scale_y_continuous(limits=c(-0.2,0.5)) +
  theme_minimal()+theme(
    axis.text.y=element_blank(),
    axis.ticks.y=element_blank()) +
  labs(y='evidence',
       x='')+
  theme(legend.position = 'none');
  
  ggsave(paste('./figures/RC/sim/detection_decision.png',sep='/'),
         width=3.5,height=3)
  
  sim$signal_decision_kernel %>%
    group_by(subj_id,side,eccentricity) %>%
    summarise(evidence=mean(evidence)) %>%
    group_by(subj_id,eccentricity)%>%
    summarise(rel_evidence = evidence[side=='true']-evidence[side=='opposite'],
              sum_evidence = evidence[side=='true']+evidence[side=='opposite']) %>%
    pivot_longer(cols = ends_with('evidence'),names_to='contrast',values_to='evidence')%>%
    group_by(contrast,eccentricity) %>%
    summarise(se=se(evidence),
              evidence=mean(evidence)) %>%
    ggplot(aes(x=eccentricity,y=evidence,fill=contrast, shape=eccentricity)) +
    geom_abline(intercept=0, slope=0,size=1) +
    geom_errorbar(aes(ymin=evidence-se,
                      ymax=evidence+se), 
                  color='black')+
    geom_point(size=6) +
    scale_shape_manual(values=c(21,22,23,24))+
    scale_color_manual(values=detection_colors)+
    scale_fill_manual(values=detection_colors) +
    scale_y_continuous(limits=c(-0.2,0.5)) +
    theme_minimal()+theme(
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()) +
    labs(y='evidence',
         x='')+
    theme(legend.position = 'none');
  


  ggsave(paste('./figures/RC/sim/detection_decision_sum_rel.png',sep='/'),
         width=3.5,height=3);
  
  sim$signal_confidence_kernel %>%
    filter(response==1)%>%
    group_by(subj_id,side,eccentricity) %>%
    summarise(evidence=mean(diff)) %>%
    group_by(side,eccentricity) %>%
    summarise(se=se(evidence),
              evidence=mean(evidence))%>%
    ggplot(aes(x=eccentricity,shape=eccentricity,y=evidence,fill=side)) +
    geom_hline(yintercept=0) +
    # annotate(geom = "rect", xmin=0, xmax=300, ymin=-0.6,ymax=-0.55,
    #          color="transparent", fill="black") +
    geom_line() +
    geom_ribbon(aes(ymin = evidence-se, ymax = evidence+se, fill=side),alpha=0.5) +
    scale_color_manual(values=detection_colors)+
    scale_fill_manual(values=detection_colors) +
    scale_x_continuous(limits = xlim) +
    scale_y_continuous(limits = ylim)+
    theme_minimal()+theme(
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()) +
    labs(y='evidence',
         x='time (ms.)')+
    theme(legend.position = 'none');

  ggsave(paste('./figures/RC',exp_label,'detection_conf_yes.png',sep='/'),
         width=3.5,height=3)
  
```

We simulated 10,000 discrimination and 10,000 detection trials per model (100 trials x 100 simulated agents per model). On each discrimination trial, the signal channel could be right or left with equal probability. On half of the detection trials both channels were noise channels. We then sampled, for each trial, 12 values from each channel. These 24 values were then passed on to the simulated agent, who returned a decision and a confidence rating. We then subjected the agents’ decisions and confidence ratings to a reverse correlation analysis. We now turn to describe this analysis, which will also be used to analyze the behaviour of (actual!) participants in Exp. 1-4.

## Reverse correlation analysis

Following @zylberberg2012construction, we took a reverse correlation approach and asked which sources of evidence (positive, negative, relative, and sum evidence) contribute to agents’ decisions and confidence ratings. This analysis focuses on random fluctuations in signal intensity, and asks how they affect behaviour (here, decisions and confidence in these decisions). For simplicity, here we collapse across timepoints instead of correlating noise with decision or confidence per timepoint, as we do for human data.

## Methodological note: positive evidence bias in perceptual decisions

The positive evidence bias in decision confidence is often seen as particularly striking, given that positive and negative evidence are equally weighted in forming a decision [@zylberberg2012construction; @peters2017perceptual]. For example, using reverse correlation, @zylberberg2012construction showed that momentary fluctuations in the availability of perceptual evidence for and against a decision were equally predictive of the decision itself. Similarly, @peters2017perceptual showed that in classifying rapidly presented images as ‘face’ or ‘house’, decisions are not solely guided by positive evidence (e.g., face-related brain activity when deciding ‘face’), but also by negative evidence (e.g., house-related brain activity when deciding ‘face’).

In both cases, it is useful to ask what it would look like for an agent to only consider positive evidence in making a decision. This soon becomes circular, because positive and negative evidence are defined with respect to the decision itself. For example, when analyzing the decisions of an agent that consistently ignores evidence for one alternative (similar to the random attention model above), both positive and negative evidence should still be predictive of decisions. The effect of positive evidence is then driven by those trials in which the agent selected the attended alternative, and the effect of negative evidence by those trials in which the agent selected the ignored alternative (because the evidence for the attended alternative was insufficient). Put differently, asymmetries of positive and negative evidence cannot affect the decision itself, because at the time of making the decision there is no positive and negative evidence, but two sources of evidence that may become positive or negative, depending on the decision. For this reason, in measuring evidence weighting in decision formation, we defined positive and negative evidence relative to the ground truth rather than the agents’ decision.


## Discrimination decisions

First, to make sure we are measuring true random fluctuations and not systematic differences between noise and signal channel, we mean centered all signal channels to 0. ‘Sum evidence’ was defined as the total sum of noise terms across both channels $E_r+E_l$. ‘Relative evidence’ was defined as the difference in noise terms between the signal and non-signal channels ($E_r-E_l$ when $E_r$ was the signal channel and $E_l-E_r$ when $E_l$ was the signal channel). All four models predicted that the probability of correctly identifying the signal channel in the discrimination task should increase with relative evidence (Fig. 3A). The models predicted no effect of sum evidence on discrimination decisions.

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

# Open Practices Statement

The data and materials for all experiments are available at github.com/matanmazor/reverseCorrelation. All three experiments were pre-registered (Exp. 1: https://osf.io/z2s93/ ; Exp. 2: https://osf.io/d3vkm/; Exp. 3: https://osf.io/hm3fn/)
